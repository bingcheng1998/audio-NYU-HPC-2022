{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcriptions(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './data/segments/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3550"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = get_transcriptions(path+'train.txt')\n",
    "if (lines[-1]==''):\n",
    "    lines = lines[:-1]\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_line(line):\n",
    "    id, text, phoneme, note, note_duration, phoneme_duration, slur_note = line.split('|')\n",
    "    phoneme = phoneme.split(' ')\n",
    "    note = note.split(' ')\n",
    "    note_duration = [float(i) for i in note_duration.split(' ')]\n",
    "    phoneme_duration = [float(i) for i in phoneme_duration.split(' ')]\n",
    "    slur_note = [int(i) for i in slur_note.split(' ')]\n",
    "    assert len(phoneme) == len(note_duration) and len(phoneme_duration) == len(slur_note) and len(slur_note) == len(phoneme)\n",
    "    return id, text, phoneme, note, note_duration, phoneme_duration, slur_note\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_set = set()\n",
    "note_set = set()\n",
    "for line in lines:\n",
    "    id, text, phoneme, note, note_duration, phoneme_duration, slur_note = parser_line(line)\n",
    "    phoneme_set.update(set(phoneme))\n",
    "    note_set.update(set(note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phoneme_set, note_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2099003712|我的爱已降落|w o SP d e ai y i SP j iang l uo|F4 F4 rest F4 F4 E4 D4 D4 rest E4 E4 F4 F4|0.263560 0.263560 0.078530 0.448900 0.448900 0.291180 1.209190 1.209190 0.805550 0.854130 0.854130 1.851400 1.851400|0.1313 0.13226 0.07853 0.14925 0.29965 0.29118 0.2753 0.93389 0.80555 0.10491 0.74922 0.28334 1.56806|0 0 0 0 0 0 0 0 0 0 0 0 0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = lines[3549]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "id, text, phoneme, note, note_duration, phoneme_duration, slur_note = parser_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('我的爱已降落',\n",
       " ['wo', 'SP', 'de', 'ai', 'yi', 'SP', 'jiang', 'luo'],\n",
       " ['F4', 'rest', 'F4', 'E4', 'D4', 'rest', 'E4', 'F4'],\n",
       " [0.26356, 0.07853, 0.4489, 0.29118, 1.20919, 0.80555, 0.85413, 1.8514])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_table = ['b', 'p', 'm', 'f',\n",
    "                'd', 't', 'n', 'l',\n",
    "                'g', 'k', 'h',\n",
    "                'j', 'q', 'x',\n",
    "                'zh', 'ch', 'sh', 'r', 'z', 'c', 's', 'y', 'w']\n",
    "def merge_note(text, phoneme, note, note_duration):\n",
    "    # remove the duplicate items in phoneme, note, and note_duration\n",
    "    # use text to verify the length\n",
    "    phoneme = phoneme.copy()\n",
    "    note = note.copy()\n",
    "    note_duration = note_duration.copy()\n",
    "    for i in range(len(phoneme)-1, 0, -1):\n",
    "        if note_duration[i] == note_duration[i-1] and phoneme[i-1] in initial_table:\n",
    "            del note_duration[i]\n",
    "            del note[i]\n",
    "            phoneme[i-1]=phoneme[i-1]+phoneme[i]\n",
    "            del phoneme[i]\n",
    "    phoneme_filtered = [i for i in phoneme if i not in ['AP','SP']]\n",
    "    assert len(text) == len(phoneme_filtered), f'len(text):{len(text)} ({text}) != len(phoneme): {len(phoneme_filtered)} ({phoneme_filtered})'\n",
    "    return text, phoneme, note, note_duration\n",
    "merge_note(text, phoneme, note, note_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [id, text, phoneme, note, note_duration, phoneme_duration, slur_note]:\n",
    "#     print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(id, path, sr = 16000):\n",
    "    wav_path = path+'wavs/'+id+'.wav'\n",
    "    waveform, sample_rate = torchaudio.load(wav_path)\n",
    "    if sample_rate != sr:\n",
    "        waveform = torchaudio.functional.resample(waveform[0].unsqueeze(0), sample_rate, sr)\n",
    "    return waveform\n",
    "\n",
    "waveform = get_audio(id, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.8633699999999997, 3.863375)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(phoneme_duration), waveform.shape[-1]/16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "def melspec(waveform, sr=16000):\n",
    "    n_fft = 1024\n",
    "    win_length = None\n",
    "    hop_length = 512\n",
    "    n_mels = 128\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=sr,\n",
    "        n_fft=n_fft,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "        center=True,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm='slaney',\n",
    "        onesided=True,\n",
    "        n_mels=n_mels,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "    melspec = mel_spectrogram(waveform)\n",
    "    return melspec\n",
    "\n",
    "def plot_alignment(waveform, text, phoneme, note, note_duration, phoneme_duration, slur_note, sr=16000):\n",
    "    text, phoneme_merge, note, note_duration = merge_note(text, phoneme, note, note_duration)\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(16, 10))\n",
    "    for a in ax:\n",
    "        a.set_xticks([])\n",
    "        a.set_yticks([])\n",
    "    [ax1, ax2, ax3] = ax\n",
    "    # ax1 wave form\n",
    "    ratio = sr\n",
    "    ax1.plot(waveform/(max(torch.max(waveform), -torch.min(waveform)))*0.8)\n",
    "    ax1.set_xlim(0, waveform.size(-1))\n",
    "    ax1.set_ylim(-1.0, 1.0)\n",
    "    time_current = 0.\n",
    "    for i in range(len(phoneme_duration)):\n",
    "        x0 = ratio * time_current\n",
    "        time_current += phoneme_duration[i]\n",
    "        x1 = ratio * time_current\n",
    "        ax1.axvspan(x0, x1, ymin=0.5, ymax=1, alpha=0.1, color=\"red\")\n",
    "        ax1.annotate(phoneme[i], (x0, 0.85), color='black', fontsize=10)\n",
    "    time_current = 0\n",
    "    for i in range(len(phoneme_duration)):\n",
    "        x0 = ratio * time_current\n",
    "        time_current += phoneme_duration[i]\n",
    "        x1 = ratio * time_current\n",
    "        ax1.axvspan(x0, x1, ymin=0, ymax=0.5, alpha=0.1, color=\"blue\")\n",
    "        ax1.annotate(note[i].split('/')[0], (x0, -0.85), color='black', fontsize=10)\n",
    "\n",
    "    \n",
    "    mel = melspec(waveform)\n",
    "    ax2.imshow(librosa.power_to_db(mel) , origin='lower', aspect='auto')\n",
    "    print(mel.shape)\n",
    "    # ax2.set_xlim(0, mel.size(-1))\n",
    "\n",
    "    ax3.set_ylim(-1.0, 1.0)\n",
    "    ax3.set_xlim(0, waveform.size(-1))\n",
    "    y_split = lambda k: [1.0*(i)/k for i in range(k)]\n",
    "    k = 6\n",
    "    ys = y_split(k)\n",
    "    for i in range(k):\n",
    "        ax3.axhline(y=ys[i]*2-1)\n",
    "    # notes *3 \n",
    "    time_current = 0\n",
    "    for i in range(len(note_duration)):\n",
    "        x0 = ratio * time_current\n",
    "        time_current += note_duration[i]\n",
    "        x1 = ratio * time_current\n",
    "        ax3.axvline(x=x1, ymin=ys[3], ymax=1, color=\"blue\")\n",
    "        ax3.annotate(note[i].split('/')[0], (x0, 0.8), color='black', fontsize=10)\n",
    "        ax3.annotate(note_duration[i], (x0, 0.5), color='black', fontsize=10)\n",
    "        ax3.annotate(phoneme_merge[i], (x0, 0.1), color='black', fontsize=10)\n",
    "    phoneme *2\n",
    "    time_current = 0\n",
    "    for i in range(len(phoneme_duration)):\n",
    "        x0 = ratio * time_current\n",
    "        time_current += phoneme_duration[i]\n",
    "        x1 = ratio * time_current\n",
    "        ax3.axvline(x=x1, ymin=ys[1], ymax=1, color=\"red\")\n",
    "        # ax3.annotate(note[i].split('/')[0], (x0, 0.8), color='black', fontsize=10)\n",
    "        # ax3.annotate(note_duration[i], (x0, 0.5), color='black', fontsize=10)\n",
    "        # ax3.annotate(slur_note[i], (x0, 0.1), color='black', fontsize=10)\n",
    "        ax3.annotate(phoneme[i], (x0, -0.2), color='black', fontsize=14)\n",
    "        ax3.annotate(phoneme_duration[i], (x0, -0.5), color='black', fontsize=14)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "\n",
    "plot_alignment(waveform[0], text, phoneme, note, note_duration, phoneme_duration, slur_note)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3322949f56cb4db99427e05ed2d4a87f0497ffa3e41dd81b99d577253bd3be5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
