{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chinese2pinyin(text):\n",
    "    pinyin = lazy_pinyin(text, strict=True,errors=lambda x: u'')\n",
    "    pinyin = [i for i in '|'.join(pinyin)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import dtype\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchaudio\n",
    "\n",
    "class AiShellDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, sample_rate=16000, transform=None):\n",
    "        transcript_file = data_path+'transcript/aishell_transcript_v0.8.txt'\n",
    "        self.transcript = self.gen_transcript(transcript_file)\n",
    "        self.wav_files = self.get_all_wav_files(data_path, self.transcript)\n",
    "        self.dataset_file_num = len(self.wav_files)\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.sample_rate = sample_rate\n",
    "        self.threshold = 120000 # to avoid GPU memory used out\n",
    "        self.batch_size = 80 # to avoid GPU memory used out\n",
    "        self.split_ratio = [1000, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_file_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if idx >= self.dataset_file_num:\n",
    "            return {'audio': None, 'text': None}\n",
    "        audio_name = self.wav_files[idx]\n",
    "        waveform, sample_rate = torchaudio.load(audio_name)\n",
    "        waveform = waveform\n",
    "        if sample_rate != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, self.sample_rate)\n",
    "        dict_id = audio_name.rsplit('/',1)[-1].split('.')[0]\n",
    "        audio_content = self.transcript[dict_id]\n",
    "        sample = {'audio': waveform, 'text': audio_content}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def parse_line(self, line):\n",
    "        id, text = line.split(' ', 1)\n",
    "        text = ''.join(text.split(' '))\n",
    "        return id, text\n",
    "\n",
    "    def gen_transcript(self, transcript_file):\n",
    "        transcript = {}\n",
    "        with open(transcript_file, 'r') as f:\n",
    "            content = f.read()\n",
    "            lines = content.split('\\n')[:-1]\n",
    "            for line in lines:\n",
    "                id, text = self.parse_line(line)\n",
    "                transcript[id] = text\n",
    "        return transcript\n",
    "\n",
    "    def get_all_wav_files(self, path, transcript):\n",
    "        folders = []\n",
    "        train = os.listdir(path+'wav/train/')\n",
    "        folders += [path+'wav/train/'+i for i in train]\n",
    "        dev = os.listdir(path+'wav/dev/')\n",
    "        folders += [path+'wav/dev/'+i for i in dev]\n",
    "        test = os.listdir(path+'wav/test/')\n",
    "        folders += [path+'wav/test/'+i for i in test]\n",
    "        files = []\n",
    "        for folder in folders:\n",
    "            files += [folder+'/'+i for i in os.listdir(folder) if i[:-4] in transcript]\n",
    "        return files\n",
    "    \n",
    "    def split(self, split_ratio=None, seed=42):\n",
    "        audio_dataset = self\n",
    "        size = len(audio_dataset)\n",
    "        my_split_ratio = self.split_ratio if split_ratio is None else split_ratio\n",
    "        lengths = [(i*size)//sum(my_split_ratio) for i in my_split_ratio]\n",
    "        lengths[-1] = size - sum(lengths[:-1])\n",
    "        split_dataset = random_split(audio_dataset, lengths, generator=torch.Generator().manual_seed(seed))\n",
    "        return split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: 140895 test_set: 705\n",
      "torch.Size([8, 94301]) torch.Size([8, 82])\n",
      "torch.Size([7, 90778]) torch.Size([7, 75])\n",
      "torch.Size([8, 95557]) torch.Size([8, 90])\n",
      "torch.Size([8, 96123]) torch.Size([8, 89])\n",
      "torch.Size([8, 102204]) torch.Size([8, 95])\n",
      "torch.Size([6, 97673]) torch.Size([6, 93])\n",
      "torch.Size([8, 88088]) torch.Size([8, 76])\n",
      "torch.Size([8, 95365]) torch.Size([8, 75])\n",
      "torch.Size([7, 77399]) torch.Size([7, 67])\n",
      "torch.Size([8, 102859]) torch.Size([8, 73])\n",
      "torch.Size([8, 93783]) torch.Size([8, 78])\n",
      "torch.Size([8, 103609]) torch.Size([8, 82])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # dataset = AudioDataset('./data/ST-CMDS-20170001_1-OS/')\n",
    "    dataset = AiShellDataset('./data/data_aishell/')\n",
    "    batch_size = 8\n",
    "    train_set, test_set = dataset.split([1000, 5])\n",
    "    k_size = 5 # kernel size for audio encoder\n",
    "    from pypinyin import lazy_pinyin\n",
    "    def chinese2pinyin(text):\n",
    "        pinyin = lazy_pinyin(text, strict=True,errors=lambda x: u'')\n",
    "        pinyin = [i for i in '|'.join(pinyin)]\n",
    "        return pinyin\n",
    "    from utils.helper import get_labels\n",
    "    from utils.dataset import LoaderGenerator\n",
    "    labels = get_labels()\n",
    "    loaderGenerator = LoaderGenerator(labels, chinese2pinyin, k_size)\n",
    "    train_loader = loaderGenerator.dataloader(train_set, batch_size)\n",
    "    test_loader = loaderGenerator.dataloader(test_set, batch_size) # without backprop, can use larger batch\n",
    "    print('train_set:', len(train_set), 'test_set:',len(test_set))\n",
    "    steps = 10\n",
    "    for i_batch, sample_batched in enumerate(test_loader):\n",
    "        print(sample_batched['audio'].shape, sample_batched['target'].shape)\n",
    "        # for i in sample_batched['audio']:\n",
    "        #     print(i.shape)\n",
    "        if steps < 0:\n",
    "            break\n",
    "        steps -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = AiShellDataset('./data/data_aishell/')\n",
    "lens = [dataset[i]['audio'].shape[-1] for i in range(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens2 = [dataset[i+500]['audio'].shape[-1] for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lens+lens2, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from numpy import dtype\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchaudio\n",
    "\n",
    "class PrimeWordsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, sample_rate=16000, transform=None):\n",
    "        with open(data_path+'set1_transcript.json') as f:\n",
    "            json_data = json.load(f)\n",
    "        self.json_data = json_data\n",
    "        # self.wav_files = self.get_all_wav_files(data_path, self.transcript)\n",
    "        self.dataset_file_num = len(self.json_data)\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.sample_rate = sample_rate\n",
    "        self.threshold = 220000 # to avoid GPU memory used out\n",
    "        self.batch_size = 40 # to avoid GPU memory used out\n",
    "        self.split_ratio = [1000, 2]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_file_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if idx >= self.dataset_file_num:\n",
    "            return {'audio': None, 'text': None}\n",
    "        audio_file, audio_content = self.parse_line(self.json_data[idx])\n",
    "        waveform, sample_rate = torchaudio.load(self.get_wav(audio_file))\n",
    "        waveform = waveform\n",
    "        if sample_rate != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, self.sample_rate)\n",
    "        sample = {'audio': waveform, 'text': audio_content}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def parse_line(self, line):\n",
    "        return line['file'], line['text']\n",
    "\n",
    "    def get_wav(self, file_name):\n",
    "        path = self.data_path+'audio_files/'+file_name[0]+'/'+file_name[:2]+'/'+file_name\n",
    "        return path\n",
    "\n",
    "    def split(self, split_ratio=None, seed=42):\n",
    "        audio_dataset = self\n",
    "        size = len(audio_dataset)\n",
    "        my_split_ratio = self.split_ratio if split_ratio is None else split_ratio\n",
    "        lengths = [(i*size)//sum(my_split_ratio) for i in my_split_ratio]\n",
    "        lengths[-1] = size - sum(lengths[:-1])\n",
    "        split_dataset = random_split(audio_dataset, lengths, generator=torch.Generator().manual_seed(seed))\n",
    "        return split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: 50648 test_set: 254\n",
      "torch.Size([8, 176636]) torch.Size([8, 102])\n",
      "torch.Size([8, 145909]) torch.Size([8, 94])\n",
      "torch.Size([8, 139198]) torch.Size([8, 92])\n",
      "torch.Size([8, 140152]) torch.Size([8, 94])\n",
      "torch.Size([8, 151359]) torch.Size([8, 98])\n",
      "torch.Size([8, 130548]) torch.Size([8, 97])\n",
      "torch.Size([8, 182392]) torch.Size([8, 108])\n",
      "torch.Size([7, 152315]) torch.Size([7, 101])\n",
      "torch.Size([8, 176638]) torch.Size([8, 92])\n",
      "torch.Size([7, 161280]) torch.Size([7, 88])\n",
      "torch.Size([8, 168951]) torch.Size([8, 112])\n",
      "torch.Size([8, 169595]) torch.Size([8, 106])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset = PrimeWordsDataset('./data/primewords_md_2018_set1/')\n",
    "    batch_size = 8\n",
    "    train_set, test_set = dataset.split([1000, 5])\n",
    "    k_size = 5 # kernel size for audio encoder\n",
    "    from pypinyin import lazy_pinyin\n",
    "    def chinese2pinyin(text):\n",
    "        pinyin = lazy_pinyin(text, strict=True,errors=lambda x: u'')\n",
    "        pinyin = [i for i in '|'.join(pinyin)]\n",
    "        return pinyin\n",
    "    from utils.helper import get_labels\n",
    "    from utils.dataset import LoaderGenerator\n",
    "    labels = get_labels()\n",
    "    loaderGenerator = LoaderGenerator(labels, chinese2pinyin, k_size)\n",
    "    train_loader = loaderGenerator.dataloader(train_set, batch_size)\n",
    "    test_loader = loaderGenerator.dataloader(test_set, batch_size) # without backprop, can use larger batch\n",
    "    print('train_set:', len(train_set), 'test_set:',len(test_set))\n",
    "    steps = 10\n",
    "    for i_batch, sample_batched in enumerate(test_loader):\n",
    "        print(sample_batched['audio'].shape, sample_batched['target'].shape)\n",
    "        # for i in sample_batched['audio']:\n",
    "        #     print(i.shape)\n",
    "        if steps < 0:\n",
    "            break\n",
    "        steps -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PrimeWordsDataset('./data/primewords_md_2018_set1/')\n",
    "lens = [dataset[i]['audio'].shape[-1] for i in range(500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0003, 0.0002, 0.0001]]),\n",
       " 'text': '两碑尺寸相同 规格一致 均高六点八七米 分别由螭龙碑首'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=15\n",
    "torchaudio.save('./audio-temp.wav', dataset[i]['audio'], 16000)\n",
    "dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens2 = [dataset[i+500]['audio'].shape[-1] for i in range(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lens+lens2, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_path = './data/zhspeechocean/'\n",
    "\n",
    "\n",
    "class SpeechOceanDataset(Dataset):\n",
    "    def __init__(self, data_path, sample_rate=16000, transform=None):\n",
    "        meta_data = data_path + 'metadata.csv'\n",
    "        self.meta_data = pd.read_csv(meta_data, sep='\\t')\n",
    "        self.dataset_file_num = len(self.meta_data)\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.sample_rate = sample_rate\n",
    "        self.threshold = 90000 # to avoid GPU memory used out\n",
    "        self.batch_size = 128 # to avoid GPU memory used out\n",
    "        self.split_ratio = [100, 5]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_file_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if idx >= self.dataset_file_num:\n",
    "            return {'audio': None, 'text': None}\n",
    "        audio_name = os.path.join(self.data_path,\n",
    "                                  self.meta_data.iloc[idx, 0])\n",
    "        waveform, sample_rate = torchaudio.load(audio_name)\n",
    "        audio_content = self.meta_data.iloc[idx, 1]\n",
    "        if sample_rate != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, self.sample_rate)\n",
    "        sample = {'audio': waveform, 'text': audio_content}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def split(self, split_ratio=None, seed=42):\n",
    "        audio_dataset = self\n",
    "        size = len(audio_dataset)\n",
    "        my_split_ratio = self.split_ratio if split_ratio is None else split_ratio\n",
    "        lengths = [(i*size)//sum(my_split_ratio) for i in my_split_ratio]\n",
    "        lengths[-1] = size - sum(lengths[:-1])\n",
    "        split_dataset = random_split(audio_dataset, lengths, generator=torch.Generator().manual_seed(seed))\n",
    "        return split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: 2285 test_set: 115\n",
      "torch.Size([7, 76030]) torch.Size([7, 66])\n",
      "torch.Size([8, 85818]) torch.Size([8, 66])\n",
      "torch.Size([8, 80063]) torch.Size([8, 89])\n",
      "torch.Size([7, 56436]) torch.Size([7, 54])\n",
      "torch.Size([8, 76025]) torch.Size([8, 67])\n",
      "torch.Size([8, 79484]) torch.Size([8, 68])\n",
      "torch.Size([8, 65661]) torch.Size([8, 76])\n",
      "torch.Size([8, 67958]) torch.Size([8, 75])\n",
      "torch.Size([8, 59323]) torch.Size([8, 70])\n",
      "torch.Size([8, 86392]) torch.Size([8, 66])\n",
      "torch.Size([8, 55872]) torch.Size([8, 50])\n",
      "torch.Size([8, 52413]) torch.Size([8, 47])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset = SpeechOceanDataset('./data/zhspeechocean/')\n",
    "    batch_size = 8\n",
    "    train_set, test_set = dataset.split([100, 5])\n",
    "    k_size = 5 # kernel size for audio encoder\n",
    "    from pypinyin import lazy_pinyin\n",
    "    def chinese2pinyin(text):\n",
    "        pinyin = lazy_pinyin(text, strict=True,errors=lambda x: u'')\n",
    "        pinyin = [i for i in '|'.join(pinyin)]\n",
    "        return pinyin\n",
    "    from utils.helper import get_labels\n",
    "    from utils.dataset import LoaderGenerator\n",
    "    labels = get_labels()\n",
    "    loaderGenerator = LoaderGenerator(labels, chinese2pinyin, k_size)\n",
    "    train_loader = loaderGenerator.dataloader(train_set, batch_size)\n",
    "    test_loader = loaderGenerator.dataloader(test_set, batch_size) # without backprop, can use larger batch\n",
    "    print('train_set:', len(train_set), 'test_set:',len(test_set))\n",
    "    steps = 10\n",
    "    for i_batch, sample_batched in enumerate(test_loader):\n",
    "        print(sample_batched['audio'].shape, sample_batched['target'].shape)\n",
    "        # for i in sample_batched['audio']:\n",
    "        #     print(i.shape)\n",
    "        if steps < 0:\n",
    "            break\n",
    "        steps -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SpeechOceanDataset('./data/zhspeechocean/')\n",
    "lens = [dataset[i]['audio'].shape[-1] for i in range(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lens, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.2709e-05,\n",
       "          -7.3612e-05, -1.0139e-04]]),\n",
       " 'text': '你这个周末有时间吗？你陪我去看看手机行吗？'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=15\n",
    "torchaudio.save('./audio-temp.wav', dataset[i]['audio'], 16000)\n",
    "dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ni|zhe|ge|zhou|mo|you|shi|jian|ma|ni|pei|wo|qu|kan|kan|shou|ji|xing|ma'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chinese2pinyin(text):\n",
    "    pinyin = lazy_pinyin(text, strict=True,errors=lambda x: u'')\n",
    "    pinyin = [i for i in '|'.join(pinyin)]\n",
    "    return pinyin\n",
    "''.join(chinese2pinyin('你这个周末有时间吗？你陪我去看看手机行吗？'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CvCorpus8Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, sample_rate=16000, transform=None):\n",
    "        df1 = pd.read_csv(data_path+'validated.tsv',sep='\\t')[['path', 'sentence']]\n",
    "        # df2 = pd.read_csv(data_path+'invalidated.tsv',sep='\\t')[['path', 'sentence']]\n",
    "        # df3 = pd.read_csv(data_path+'other.tsv',sep='\\t')[['path', 'sentence']]\n",
    "        # df = pd.concat([df1, df2, df3])\n",
    "        df = df1\n",
    "        audio_path = df['path'].to_list()\n",
    "        sentence_text = df['sentence'].to_list()\n",
    "        assert len(audio_path) == len(sentence_text)\n",
    "        self.audio_path = audio_path\n",
    "        self.sentence_text = sentence_text\n",
    "        self.size = len(audio_path)\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.sample_rate = sample_rate\n",
    "        self.threshold = 170000 # to avoid GPU memory used out\n",
    "        self.batch_size = 64 # to avoid GPU memory used out\n",
    "        self.split_ratio = [100, 1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_name = self.get_audio(idx)\n",
    "        waveform, sample_rate = torchaudio.load(audio_name)\n",
    "        waveform = waveform\n",
    "        if sample_rate != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, self.sample_rate)\n",
    "        audio_content = self.get_text(idx)\n",
    "        sample = {'audio': waveform, 'text': audio_content}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def get_audio(self, x): \n",
    "        return self.data_path+'clips/'+self.audio_path[x] if x < len(self) else None\n",
    "        \n",
    "    def get_text(self, x): \n",
    "        return self.sentence_text[x] if x < len(self) else None\n",
    "    \n",
    "    def split(self, split_ratio=None, seed=42):\n",
    "        audio_dataset = self\n",
    "        size = len(audio_dataset)\n",
    "        my_split_ratio = self.split_ratio if split_ratio is None else split_ratio\n",
    "        lengths = [(i*size)//sum(my_split_ratio) for i in my_split_ratio]\n",
    "        lengths[-1] = size - sum(lengths[:-1])\n",
    "        split_dataset = random_split(audio_dataset, lengths, generator=torch.Generator().manual_seed(seed))\n",
    "        return split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: 44517 test_set: 2226\n",
      "torch.Size([8, 124029]) torch.Size([8, 71])\n",
      "torch.Size([8, 155901]) torch.Size([8, 126])\n",
      "torch.Size([8, 148988]) torch.Size([8, 128])\n",
      "torch.Size([8, 122877]) torch.Size([8, 110])\n",
      "torch.Size([8, 142838]) torch.Size([8, 107])\n",
      "torch.Size([8, 153984]) torch.Size([8, 131])\n",
      "torch.Size([8, 142079]) torch.Size([8, 121])\n",
      "torch.Size([8, 155904]) torch.Size([8, 90])\n",
      "torch.Size([8, 131712]) torch.Size([8, 93])\n",
      "torch.Size([8, 101753]) torch.Size([8, 94])\n",
      "torch.Size([8, 97920]) torch.Size([8, 89])\n",
      "torch.Size([8, 165877]) torch.Size([8, 140])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dataset = CvCorpus8Dataset('./data/cv-corpus-8.0-2022-01-19/zh-CN/')\n",
    "    batch_size = 8\n",
    "    train_set, test_set = dataset.split([100, 5])\n",
    "    k_size = 5 # kernel size for audio encoder\n",
    "    from pypinyin import lazy_pinyin\n",
    "    def chinese2pinyin(text):\n",
    "        pinyin = lazy_pinyin(text, strict=True,errors=lambda x: u'')\n",
    "        pinyin = [i for i in '|'.join(pinyin)]\n",
    "        return pinyin\n",
    "    from utils.helper import get_labels\n",
    "    from utils.dataset import LoaderGenerator\n",
    "    labels = get_labels()\n",
    "    loaderGenerator = LoaderGenerator(labels, chinese2pinyin, k_size)\n",
    "    train_loader = loaderGenerator.dataloader(train_set, batch_size)\n",
    "    test_loader = loaderGenerator.dataloader(test_set, batch_size) # without backprop, can use larger batch\n",
    "    print('train_set:', len(train_set), 'test_set:',len(test_set))\n",
    "    steps = 10\n",
    "    for i_batch, sample_batched in enumerate(test_loader):\n",
    "        print(sample_batched['audio'].shape, sample_batched['target'].shape)\n",
    "        # for i in sample_batched['audio']:\n",
    "        #     print(i.shape)\n",
    "        if steps < 0:\n",
    "            break\n",
    "        steps -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CvCorpus8Dataset('./data/cv-corpus-8.0-2022-01-19/zh-CN/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'audio': tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0020, -0.0029, -0.0024]]),\n",
       " 'text': '台湾北部地区家庭多以在农历年前时段包润饼则是在清明期间。'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=18\n",
    "torchaudio.save('./audio-temp.wav', dataset[i]['audio'], 16000)\n",
    "dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3322949f56cb4db99427e05ed2d4a87f0497ffa3e41dd81b99d577253bd3be5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
