{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_log(file_name, log, mode='a', path = './checkpoint/'):\n",
    "#     with open(path+file_name, mode) as f:\n",
    "#         if mode == 'a':\n",
    "#             f.write('\\n')\n",
    "#         if type(log) is str:\n",
    "#             f.write(log)\n",
    "#             print(log)\n",
    "#         else:\n",
    "#             log = [str(l) for l in log]\n",
    "#             f.write(' '.join(log))\n",
    "#             print(' '.join(log))\n",
    "\n",
    "# save_log('test.txt', ['I like',100,'l'], 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchaudio\n",
    "from pypinyin import lazy_pinyin, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.WAV2VEC2_XLSR53\n",
    "wave2vec_model = bundle.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wave2vec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_size = wave2vec_model.feature_extractor.conv_layers[0].conv.kernel_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wave2vec_model.encoder.transformer.layers[-1].final_layer_norm.normalized_shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChineseStt(torch.nn.Module):\n",
    "    def __init__(self, wave2vec_model, out_features):\n",
    "        super(ChineseStt, self).__init__()\n",
    "        self.feature_extractor = wave2vec_model.feature_extractor\n",
    "        self.encoder = wave2vec_model.encoder\n",
    "        in_features = wave2vec_model.encoder.transformer.layers[-1].final_layer_norm.normalized_shape[0]\n",
    "        self.aux = nn.Linear(in_features, out_features, bias=True)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        x, lengths = self.feature_extractor(x, lengths)\n",
    "        x = self.encoder(x, lengths)\n",
    "        x = self.aux(x)\n",
    "        return x, lengths\n",
    "\n",
    "chineseStt = ChineseStt(wave2vec_model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChineseStt(\n",
       "  (feature_extractor): FeatureExtractor(\n",
       "    (conv_layers): ModuleList(\n",
       "      (0): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "      )\n",
       "      (1): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "      (2): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "      (3): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "      (4): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "      )\n",
       "      (5): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "      )\n",
       "      (6): ConvLayerBlock(\n",
       "        (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (feature_projection): FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (pos_conv_embed): ConvolutionalPositionalEmbedding(\n",
       "        (conv): Conv1d(1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): EncoderLayer(\n",
       "          (attention): SelfAttention(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): FeedForward(\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (aux): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chineseStt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "SPEECH_URL = \"https://download.pytorch.org/torchaudio/tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
    "SPEECH_FILE = \"_assets/speech.wav\"\n",
    "\n",
    "if not os.path.exists(SPEECH_FILE):\n",
    "    os.makedirs(\"_assets\", exist_ok=True)\n",
    "    with open(SPEECH_FILE, \"wb\") as file:\n",
    "        file.write(requests.get(SPEECH_URL).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 54400])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.inference_mode():\n",
    "    waveform, _ = torchaudio.load(SPEECH_FILE)\n",
    "    print(waveform.shape)\n",
    "    emissions, _ = chineseStt(waveform)\n",
    "    emissions = torch.log_softmax(emissions, dim=-1)\n",
    "\n",
    "emission = emissions[0].cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypinyin import lazy_pinyin, Style\n",
    "\n",
    "\n",
    "def chinese2pinyin(text):\n",
    "    pinyin = lazy_pinyin(text, strict=True,errors=lambda x: u'')\n",
    "    pinyin = [i for i in '|'.join(pinyin)]\n",
    "    return pinyin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.dataset' from '/scratch/bh2283/code/utils/dataset.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from utils.dataset import AudioDataset, LoaderGenerator\n",
    "import importlib\n",
    "importlib.reload(sys.modules['utils.dataset'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.helper import get_labels\n",
    "labels = get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 78600]) torch.Size([8, 57])\n"
     ]
    }
   ],
   "source": [
    "dataset = AudioDataset('./data/ST-CMDS-20170001_1-OS/')\n",
    "train_set, test_set = dataset.split([8, 2])\n",
    "loaderGenerator = LoaderGenerator(labels, chinese2pinyin, k_size)\n",
    "train_loader = loaderGenerator.dataloader(train_set, 8)\n",
    "test_loader = loaderGenerator.dataloader(test_set, 8)\n",
    "for data in test_loader:\n",
    "    print(data['audio'].shape, data['target'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.textDecoder import GreedyCTCDecoder, NaiveDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = lambda x: sum(x)/len(x)\n",
    "mean([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102600\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchaudio\n",
    "\n",
    "from utils.dataset import AudioDataset, LoaderGenerator\n",
    "importlib.reload(sys.modules['utils.dataset'])\n",
    "\n",
    "dataset = AudioDataset('./data/ST-CMDS-20170001_1-OS/')\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clips\t invalidated.tsv  reported.tsv\ttrain.tsv\n",
      "dev.tsv  other.tsv\t  test.tsv\tvalidated.tsv\n"
     ]
    }
   ],
   "source": [
    "path = './data/cv-corpus-8.0-2022-01-19/zh-CN/'\n",
    "!ls ./data/cv-corpus-8.0-2022-01-19/zh-CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65479"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(path+'clips')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accents</th>\n",
       "      <th>locale</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...</td>\n",
       "      <td>common_voice_zh-CN_19703883.mp3</td>\n",
       "      <td>模式种采样自台湾龟山岛。</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thirties</td>\n",
       "      <td>female</td>\n",
       "      <td>出生地：31 上海市</td>\n",
       "      <td>zh-CN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...</td>\n",
       "      <td>common_voice_zh-CN_19706151.mp3</td>\n",
       "      <td>后者娶天之瓮主神。</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>thirties</td>\n",
       "      <td>female</td>\n",
       "      <td>出生地：31 上海市</td>\n",
       "      <td>zh-CN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>faa0ecc626e80638016d6295b5018372b8567f5f3177f6...</td>\n",
       "      <td>common_voice_zh-CN_19961025.mp3</td>\n",
       "      <td>贝尔卢。</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>出生地：32 江苏省</td>\n",
       "      <td>zh-CN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           client_id  \\\n",
       "0  25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...   \n",
       "1  25bc975d06200b7b1c9135db090561cb0d9b28d172e51c...   \n",
       "2  faa0ecc626e80638016d6295b5018372b8567f5f3177f6...   \n",
       "\n",
       "                              path      sentence  up_votes  down_votes  \\\n",
       "0  common_voice_zh-CN_19703883.mp3  模式种采样自台湾龟山岛。         1           0   \n",
       "1  common_voice_zh-CN_19706151.mp3     后者娶天之瓮主神。         1           0   \n",
       "2  common_voice_zh-CN_19961025.mp3          贝尔卢。         1           0   \n",
       "\n",
       "        age  gender     accents locale segment  \n",
       "0  thirties  female  出生地：31 上海市  zh-CN     NaN  \n",
       "1  thirties  female  出生地：31 上海市  zh-CN     NaN  \n",
       "2  twenties    male  出生地：32 江苏省  zh-CN     NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_csv(path+'other.tsv',sep='\\t')\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46743 6424 12312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "65479"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(path+'validated.tsv',sep='\\t')\n",
    "df2 = pd.read_csv(path+'invalidated.tsv',sep='\\t')\n",
    "df3 = pd.read_csv(path+'other.tsv',sep='\\t')\n",
    "print(len(df1),len(df2),len(df3))\n",
    "len(set(df1.path.to_list()+df2.path.to_list()+df3.path.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.path.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>common_voice_zh-CN_22069600.mp3</td>\n",
       "      <td>宋朝末年年间定居粉岭围。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>common_voice_zh-CN_22006851.mp3</td>\n",
       "      <td>渐渐行动不便</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>common_voice_zh-CN_22115132.mp3</td>\n",
       "      <td>二十一年去世。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>common_voice_zh-CN_22120171.mp3</td>\n",
       "      <td>他们自称恰哈拉。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>common_voice_zh-CN_18646658.mp3</td>\n",
       "      <td>局部干涩的例子包括有口干、眼睛干燥、及阴道干燥。</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              path                  sentence\n",
       "0  common_voice_zh-CN_22069600.mp3              宋朝末年年间定居粉岭围。\n",
       "1  common_voice_zh-CN_22006851.mp3                    渐渐行动不便\n",
       "2  common_voice_zh-CN_22115132.mp3                   二十一年去世。\n",
       "3  common_voice_zh-CN_22120171.mp3                  他们自称恰哈拉。\n",
       "4  common_voice_zh-CN_18646658.mp3  局部干涩的例子包括有口干、眼睛干燥、及阴道干燥。"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(path+'validated.tsv',sep='\\t')[['path', 'sentence']]\n",
    "df2 = pd.read_csv(path+'invalidated.tsv',sep='\\t')[['path', 'sentence']]\n",
    "df3 = pd.read_csv(path+'other.tsv',sep='\\t')[['path', 'sentence']]\n",
    "df = pd.concat([df1, df2, df3])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65479, 65479)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path = df['path']\n",
    "sentence_text = df['sentence']\n",
    "len(audio_path), len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        common_voice_zh-CN_22069600.mp3\n",
       "1        common_voice_zh-CN_22006851.mp3\n",
       "2        common_voice_zh-CN_22115132.mp3\n",
       "3        common_voice_zh-CN_22120171.mp3\n",
       "4        common_voice_zh-CN_18646658.mp3\n",
       "                      ...               \n",
       "12307    common_voice_zh-CN_30532477.mp3\n",
       "12308    common_voice_zh-CN_30532478.mp3\n",
       "12309    common_voice_zh-CN_30532479.mp3\n",
       "12310    common_voice_zh-CN_30532481.mp3\n",
       "12311    common_voice_zh-CN_30532480.mp3\n",
       "Name: path, Length: 65479, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65479, 65153, 326)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class CvCorpus8Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path, sample_rate=16000, transform=None):\n",
    "        df1 = pd.read_csv(path+'validated.tsv',sep='\\t')[['path', 'sentence']]\n",
    "        df2 = pd.read_csv(path+'invalidated.tsv',sep='\\t')[['path', 'sentence']]\n",
    "        df3 = pd.read_csv(path+'other.tsv',sep='\\t')[['path', 'sentence']]\n",
    "        df = pd.concat([df1, df2, df3])\n",
    "        audio_path = df['path'].to_list()\n",
    "        sentence_text = df['sentence'].to_list()\n",
    "        assert len(audio_path) == len(sentence_text)\n",
    "        self.audio_path = audio_path\n",
    "        self.sentence_text = sentence_text\n",
    "        self.size = len(audio_path)\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.sample_rate = sample_rate\n",
    "        self.threshold = 170000 # to avoid GPU memory used out\n",
    "        self.batch_size = 64 # to avoid GPU memory used out\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio_name = self.get_audio(idx)\n",
    "        waveform, sample_rate = torchaudio.load(audio_name)\n",
    "        waveform = waveform\n",
    "        if sample_rate != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sample_rate, self.sample_rate)\n",
    "        audio_content = self.get_text(idx)\n",
    "        sample = {'audio': waveform, 'text': audio_content}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def get_audio(self, x): \n",
    "        return self.data_path+'clips/'+self.audio_path[x] if x < len(self) else None\n",
    "        \n",
    "    def get_text(self, x): \n",
    "        return self.sentence_text[x] if x < len(self) else None\n",
    "    \n",
    "    def split(self, split_ratio=[8, 2], seed=42):\n",
    "        lengths = [(i*len(self))//sum(split_ratio) for i in split_ratio]\n",
    "        lengths[-1] = len(self) - sum(lengths[:-1])\n",
    "        split_dataset = random_split(self, lengths, generator=torch.Generator().manual_seed(seed))\n",
    "        return split_dataset\n",
    "\n",
    "cvCorpus8Dataset = CvCorpus8Dataset('./data/cv-corpus-8.0-2022-01-19/zh-CN/')\n",
    "train_set, test_set = cvCorpus8Dataset.split([1000, 5])\n",
    "loaderGenerator = LoaderGenerator(labels, chinese2pinyin, k_size)\n",
    "batch_size = 8\n",
    "train_loader = loaderGenerator.dataloader(train_set, batch_size)\n",
    "test_loader = loaderGenerator.dataloader(test_set, batch_size)\n",
    "len(cvCorpus8Dataset), len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/scratch/bh2283/code/test.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/code/test.ipynb#ch0000022vscode-remote?line=0'>1</a>\u001b[0m torchaudio\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39maudio-temp.wav\u001b[39m\u001b[39m'\u001b[39m, test_set[\u001b[39m2\u001b[39m][\u001b[39m'\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m16000\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/code/test.ipynb#ch0000022vscode-remote?line=1'>2</a>\u001b[0m test_set[\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39;49mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "torchaudio.save('audio-temp.wav', test_set[2]['audio'], 16000)\n",
    "test_set[2].decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(k):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i in range(k):\n",
    "            sample = test_set[i]\n",
    "            print(i, sample['audio'].shape, sample['text'])\n",
    "            waveform = sample['audio']\n",
    "            emissions, _ = model(waveform.to(device))\n",
    "            emissions = torch.log_softmax(emissions, dim=-1)\n",
    "            emission = emissions[0].cpu().detach()\n",
    "            transcript = decoder(emission)\n",
    "            print('transcript:', transcript, NaiveCTCDecoder(labels)(emission))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2label(idcs):\n",
    "    return [labels[i] for i in idcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 161269])\n",
      "tensor([19, 14,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
      "wu----------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data['audio'].shape)\n",
    "    torchaudio.save('audio-temp.wav', data['audio'][0].unsqueeze(0), 16000)\n",
    "    print(data['target'][0])\n",
    "    print(''.join(id2label(data['target'][0])))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('模式种采样自台湾龟山岛。', 'mo|shi|zhong|cai|yang|zi|tai|wan|gui|shan|dao')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chinese2pinyin(text):\n",
    "    pinyin = lazy_pinyin(text, strict=True,errors=lambda x: u'')\n",
    "    pinyin = [i for i in '|'.join(pinyin)]\n",
    "    return pinyin\n",
    "\n",
    "text = cvCorpus8Dataset[0]['text']\n",
    "text, ''.join(chinese2pinyin(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [train_set[i]['audio'].shape[1] for i in range(800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens2 = [train_set[i+800]['audio'].shape[1] for i in range(800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens3 = [train_set[i+8000]['audio'].shape[1] for i in range(800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens4 = [train_set[i+10000]['audio'].shape[1] for i in range(800)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(lens+lens2+lens3+lens4, bins = 50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: 12250 test_set: 62\n",
      "torch.Size([80, 162040]) torch.Size([80, 138])\n",
      "torch.Size([80, 158200]) torch.Size([80, 270])\n",
      "torch.Size([80, 157818]) torch.Size([80, 116])\n",
      "torch.Size([80, 161277]) torch.Size([80, 115])\n",
      "torch.Size([80, 162430]) torch.Size([80, 140])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 80\n",
    "k_size = 10 # kernel size for audio encoder\n",
    "from pypinyin import lazy_pinyin\n",
    "def chinese2pinyin(text):\n",
    "    pinyin = lazy_pinyin(text, strict=True,errors=lambda x: u'')\n",
    "    pinyin = [i for i in '|'.join(pinyin)]\n",
    "    return pinyin\n",
    "from utils.helper import get_labels\n",
    "labels = get_labels()\n",
    "loaderGenerator = LoaderGenerator(labels, chinese2pinyin, k_size)\n",
    "train_loader = loaderGenerator.dataloader(train_set, batch_size)\n",
    "test_loader = loaderGenerator.dataloader(test_set, batch_size) # without backprop, can use larger batch\n",
    "print('train_set:', len(train_set), 'test_set:',len(test_set))\n",
    "steps = 3\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(sample_batched['audio'].shape, sample_batched['target'].shape)\n",
    "    # for i in sample_batched['audio']:\n",
    "    #     print(i.shape)\n",
    "    if steps < 0:\n",
    "        break\n",
    "    steps -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3322949f56cb4db99427e05ed2d4a87f0497ffa3e41dd81b99d577253bd3be5"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
