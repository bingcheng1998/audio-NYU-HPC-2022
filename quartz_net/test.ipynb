{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import OpencpopDataset, MusicLoaderGenerator\n",
    "from helper import parser_line, merge_note, get_pitch_labels, get_transposed_phoneme_labels, print_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = [['a', 'b'], ['AP']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cc:\n",
    "    if 'A' in i:\n",
    "        print('----------ERROR2---------')\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®é›†åŠ è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_transform(sample, sample_rate=None):\n",
    "    id, text, phoneme, note, note_duration, phoneme_duration, slur_note = parser_line(sample['text'])\n",
    "    text_with_p, phoneme, note, note_duration, slur_note = merge_note(text, phoneme, note, note_duration, slur_note)\n",
    "    sample['chinese'] = text_with_p\n",
    "    sample['phoneme'] = phoneme\n",
    "    sample['note'] = note\n",
    "    sample['duration'] = note_duration\n",
    "    sample['slur'] = slur_note\n",
    "    return sample\n",
    "\n",
    "dataset = OpencpopDataset('/scratch/bh2283/data/opencpop/segments/', transform=dataset_transform, sample_rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3744, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = dataset.split()\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: 3744 test_set: 12\n",
      "['å¾—', 'è¿‡', 'ä¸”', 'è¿‡', 'æ˜¯', 'æˆ‘', 'å¦‚', 'ä»Š', 'é€Ÿ', 'å†™', 'SP', 'AP', 'SP', 'çˆ±', 'æ˜¯', 'ä¸', 'æ˜¯', 'AP', 'ä¸', 'å¼€', 'å£', 'æ‰', '~', 'ç', 'è´µ', '~', 'AP', 'æ‰“', 'ä»', 'å¿ƒ', 'é‡Œ', 'æš–', 'æš–', 'çš„', 'SP', 'AP', 'ç™½', 'è‰²', 'çš„', 'çº¦', 'SP', 'å®š', 'AP', 'æˆ‘', 'ä¸', 'æ„¿', 'é†’', 'è¿‡', 'æ¥', '~', '~', 'AP']\n",
      "torch.Size([52, 2])\n",
      "torch.Size([52, 2])\n",
      "torch.Size([52])\n",
      "tensor([ 5,  9,  7,  9,  7,  9,  6,  9, 30, 35, 18, 15,  4,  8, 24,  6, 31, 10,\n",
      "         8, 19,  9, 27, 10, 22, 15, 69, 12,  8, 12, 12, 12, 12, 16, 21, 84, 10,\n",
      "        15, 28, 16, 12,  6, 43, 17, 13,  9,  9, 24, 15, 12,  6, 64, 15])\n",
      "torch.Size([52, 80, 169])\n",
      "torch.Size([52]) tensor([ 12,  19,  15,  19,  16,  19,  13,  20,  62,  72,  38,  32,   9,  17,\n",
      "         50,  14,  63,  21,  17,  40,  20,  56,  21,  46,  32, 139,  26,  18,\n",
      "         25,  26,  26,  26,  34,  43, 169,  22,  31,  57,  34,  25,  13,  87,\n",
      "         35,  28,  20,  19,  49,  31,  25,  14, 130,  32])\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "note_labels = get_pitch_labels()\n",
    "phoneme_labels = get_transposed_phoneme_labels()\n",
    "slur_labels = [0, 1]\n",
    "# 0-1 åˆ†è¾¨ç‡0.01ï¼Œ1-2 åˆ†è¾¨ç‡0.05ï¼Œ2-7 åˆ†è¾¨ç‡0.2\n",
    "duration_labels = [i for i in range(7)]\n",
    "\n",
    "labels = (\n",
    "    phoneme_labels,\n",
    "    note_labels,\n",
    "    slur_labels\n",
    ")\n",
    "loaderGenerator = MusicLoaderGenerator(labels)\n",
    "train_loader = loaderGenerator.dataloader(train_set, batch_size=BATCH_SIZE)\n",
    "print('train_set:', len(train_set), 'test_set:',len(test_set))\n",
    "steps = 1\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    if steps <= 0:\n",
    "        break\n",
    "\n",
    "    print(sample_batched['chinese'])\n",
    "    print(sample_batched['phoneme'].shape)\n",
    "    print(sample_batched['phoneme_pre'].shape)\n",
    "    print(sample_batched['note_post'].shape)\n",
    "    print(sample_batched['audio_duration_quant'])\n",
    "    print(sample_batched['mel'].shape)\n",
    "    print(sample_batched['mel_len'].shape, sample_batched['mel_len'])\n",
    "    steps -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model è®¾è®¡\n",
    "\n",
    "- å°è¯•ä½¿ç”¨é€†å·ç§¯ï¼Œä¸Šé‡‡æ ·å¾—åˆ°æ‰€éœ€çš„éŸ³é¢‘\n",
    "- æˆ‘ä»¬æ— éœ€å»è®¡ç®—æ—¶é—´åœæ­¢ç¬¦ï¼Œåªéœ€è¦åœ¨è¾“å‡ºçš„æ—¶é—´å†…è®¡ç®—losså¹¶ä¸”æœ€å°åŒ–å³å¯\n",
    "- è®¾å®šä¸€ä¸ªæœ€å¤§æ—¶é—´é•¿åº¦ï¼Œæ¯”å¦‚ä¸¤ç§’ï¼Œè¶…è¿‡çš„å°±ä¸è¦äº†ï¼ˆç”¨é˜ˆå€¼ç­›æ‰ï¼‰\n",
    "- å¤šå±‚ä¸Šé‡‡æ ·å¾—åˆ°æœ€ä½³çš„è¾“å‡º\n",
    "- ä½¿ç”¨æ¢…å°”é¢‘è°±ï¼Œè¿˜æœ‰è§£ç å™¨ï¼Œå¯ä»¥ä½¿å¾—è¾“å‡ºéŸ³è´¨æ¯”stftå¥½ï¼ˆçŒœçš„ï¼Œéœ€è¦éªŒè¯ï¼‰ä¸€èˆ¬æœºå™¨å­¦ä¹ å£°ç å™¨éƒ½ä¼šå¥½ç‚¹\n",
    "\n",
    "ä½†æ˜¯æœ‰é—®é¢˜ï¼š\n",
    "- ä½¿ç”¨é€†å·ç§¯å¤ªè¿‡åˆšç›´ï¼Œæ²¡æœ‰å˜åŒ–æ€§ï¼Œå¯¼è‡´æ— æ³•å¾ˆå¥½çš„æ”¶æ•›\n",
    "- ä¸€èˆ¬é€†å·ç§¯å’ŒGANä¸€èµ·ä½¿ç”¨ï¼Œç”¨åˆ¤åˆ«å™¨å–ä»£åˆšç›´çš„loss\n",
    "- ä½¿ç”¨tacotronæ¨¡å¼å°±ä¼šå¥½å¾ˆå¤šï¼Œæ— éœ€GANï¼Œè‡ªæ”¶æ•›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.tacotron2 import Tacotron2, _get_mask_from_lengths, _Decoder, _Encoder, _Postnet\n",
    "from torchaudio.pipelines._tts.utils import _get_taco_params\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple, List, Optional, Union, overload\n",
    "\n",
    "class TacotronTail(Tacotron2):\n",
    "    def __init__(\n",
    "        self,\n",
    "        labels_lens: dict,\n",
    "        decoder = None,\n",
    "        postnet = None,\n",
    "    ) -> None:\n",
    "        _tacotron2_params=_get_taco_params(n_symbols=5) # ignore n_symbols, encoder not used \n",
    "        super().__init__(**_tacotron2_params)\n",
    "\n",
    "        embedding_dim = _tacotron2_params['encoder_embedding_dim']\n",
    "        self.embeddings = {\n",
    "            key: torch.nn.Embedding(value, embedding_dim)\n",
    "            for key, value in labels_lens.items()\n",
    "        }\n",
    "        self.embedding_register = torch.nn.ModuleList(self.embeddings.values())\n",
    "        # å°†embeddingæ³¨å†Œè¿›æ¨¡å‹ï¼Œä¸ç¡®å®šæ˜¯å¦å¤åˆ¶ï¼Œéœ€è¦åœ¨å®è·µä¸­æµ‹è¯•\n",
    "        if decoder is not None:\n",
    "            self.decoder: _Decoder = decoder\n",
    "        if postnet is not None:\n",
    "            self.postnet: _Postnet = postnet\n",
    "        self.reduce_phoneme = lambda x: torch.sum(x, 1) if len(x.shape)==3 else x\n",
    "        self.decoder.decoder_max_step = int(4 * 22050 / 256)\n",
    "        self.version = '0.01'\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: dict,\n",
    "    ):\n",
    "        embedded_inputs = [\n",
    "            self.reduce_phoneme(self.embeddings[key](inputs[key])) for key in self.embeddings.keys()\n",
    "        ]\n",
    "        embedded_inputs = torch.stack(embedded_inputs).sum(0).unsqueeze(1) # [bs, 1, emb_size]\n",
    "        # print('embedded_inputs', embedded_inputs.shape)\n",
    "        mel_specgram = inputs['mel'] # (n_batch, ``n_mels``, max of ``mel_specgram_lengths``)\n",
    "        # print('mel_specgram', mel_specgram.shape)\n",
    "        mel_specgram_lengths = inputs['mel_len']\n",
    "        mel_specgram, gate_outputs, alignments = self.decoder(\n",
    "            embedded_inputs, mel_specgram, memory_lengths=torch.ones_like(mel_specgram_lengths),\n",
    "        )\n",
    "\n",
    "        mel_specgram_postnet = self.postnet(mel_specgram)\n",
    "        mel_specgram_postnet = mel_specgram + mel_specgram_postnet\n",
    "\n",
    "        if self.mask_padding:\n",
    "            mask = _get_mask_from_lengths(mel_specgram_lengths)\n",
    "            mask = mask.expand(self.n_mels, mask.size(0), mask.size(1))\n",
    "            mask = mask.permute(1, 0, 2)\n",
    "\n",
    "            mel_specgram.masked_fill_(mask, 0.0)\n",
    "            mel_specgram_postnet.masked_fill_(mask, 0.0)\n",
    "            gate_outputs.masked_fill_(mask[:, 0, :], 1e3)\n",
    "\n",
    "        return mel_specgram, mel_specgram_postnet, gate_outputs, alignments\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(\n",
    "        self, \n",
    "        inputs: dict,\n",
    "        ) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "\n",
    "        embedded_inputs = [\n",
    "            self.reduce_phoneme(self.embeddings[key](inputs[key])) for key in self.embeddings.keys()\n",
    "        ]\n",
    "        embedded_inputs = torch.stack(embedded_inputs).sum(0).unsqueeze(1) # [bs, 1, emb_size]\n",
    "        # print('embedded_inputs', embedded_inputs.shape)\n",
    "        \n",
    "        n_batch = embedded_inputs.shape[0]\n",
    "        mel_specgram, mel_specgram_lengths, _, alignments = \\\n",
    "            self.decoder.infer(embedded_inputs, memory_lengths=torch.ones(n_batch))\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_specgram)\n",
    "        mel_outputs_postnet = mel_specgram + mel_outputs_postnet\n",
    "\n",
    "        alignments = alignments.unfold(1, n_batch, n_batch).transpose(0, 2)\n",
    "\n",
    "        return mel_outputs_postnet, mel_specgram_lengths, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/bh2283/penv/lib/python3.10/site-packages/torchaudio/models/tacotron2.py:860: UserWarning: Reached max decoder steps. The generated spectrogram might not cover the whole transcript.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "labels_lens = {\n",
    "    'audio_duration_quant': 130, # è¿™ä¸ªæ˜¯é‡åŒ–åçš„è®¡ç®—ç»“æœ\n",
    "    'phoneme': len(phoneme_labels), # æ‹¼éŸ³\n",
    "    'phoneme_pre': len(phoneme_labels), # å‰ä¸€ä¸ªæ±‰å­—çš„æ‹¼éŸ³\n",
    "    'phoneme_post': len(phoneme_labels), # åä¸€ä¸ªæ±‰å­—çš„æ‹¼éŸ³\n",
    "    'note': len(note_labels), # éŸ³è°ƒéŸ³ç¬¦\n",
    "    'note_pre': len(note_labels),\n",
    "    'note_post': len(note_labels),\n",
    "    'slur': len(slur_labels), # æ˜¯å¦ä¸ºå»¶é•¿éŸ³\n",
    "}\n",
    "model = TacotronTail(labels_lens)\n",
    "steps = 1\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    if steps <= 0:\n",
    "        break\n",
    "    model.infer(sample_batched)\n",
    "    # print(sample_batched['chinese'])\n",
    "    # print(sample_batched['phoneme'].shape)\n",
    "    # print(sample_batched['phoneme_pre'].shape)\n",
    "    # print(sample_batched['note_post'].shape)\n",
    "    # print(sample_batched['audio_duration_quant'])\n",
    "    # print(sample_batched['mel'].shape)\n",
    "    # print(sample_batched['mel_len'].shape, sample_batched['mel_len'])\n",
    "    steps -= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ\n",
    "\n",
    "- åˆå§‹å‚æ•°é…ç½®\n",
    "- è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './log/tacotron-1-'\n",
    "LEARNING_RATE = 0.001\n",
    "LOAD_PATH = './checkpoint/pre.pt'\n",
    "def save_log(file_name, log, mode='a', path = LOG_DIR):\n",
    "    with open(path+file_name, mode) as f:\n",
    "        if mode == 'a':\n",
    "            f.write('\\n')\n",
    "        if type(log) is str:\n",
    "            f.write(log)\n",
    "            print(log)\n",
    "        else:\n",
    "            log = [str(l) for l in log]\n",
    "            f.write(' '.join(log))\n",
    "            print(' '.join(log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path ./checkpoint/pre.pt exist, loading...\n"
     ]
    }
   ],
   "source": [
    "from os.path import exists\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def load_checkpoint(path):\n",
    "    if exists(path):\n",
    "        save_log(f'e.txt', ['path', path, 'exist, loading...'])\n",
    "        checkpoint = torch.load(path, map_location=device)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.decoder.load_state_dict(checkpoint['model_state_dict'], strict=False) # , strict=False?\n",
    "            model.postnet.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "\n",
    "load_checkpoint(LOAD_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.parameters()\n",
    "# params = list(model.embedding.parameters())+list(model.encoder.parameters())+list(model.speaker_encoder.parameters())\n",
    "# optimizer = torch.optim.SGD(params, lr=LEARNING_RATE, momentum=0.5)\n",
    "optimizer = torch.optim.Adam(params, lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n",
    "initial_epoch = 0\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "bce_loss = torch.nn.BCELoss()\n",
    "cos_loss = torch.nn.CosineEmbeddingLoss()\n",
    "mean = lambda x: sum(x)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_model(EPOCH, LOSS, PATH):\n",
    "    torch.save({\n",
    "            'epoch': EPOCH,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': LOSS,\n",
    "            }, PATH)\n",
    "\n",
    "def save_temp(EPOCH, LOSS):\n",
    "    PATH = f\"./checkpoint/model_temp.pt\"\n",
    "    dump_model(EPOCH, LOSS, PATH)\n",
    "    \n",
    "def save_checkpoint(EPOCH, LOSS):\n",
    "    PATH = f\"./checkpoint/model_{EPOCH}_{'%.3f' % LOSS}.pt\"\n",
    "    dump_model(EPOCH, LOSS, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch=1):\n",
    "    train_loss_q = []\n",
    "    test_loss_q = []\n",
    "    for epoch in range(initial_epoch, epoch):\n",
    "        batch_train_loss = []\n",
    "        for i_batch, sample_batched in enumerate(train_loader):\n",
    "            model.train()\n",
    "            # Step 1. Prepare Data\n",
    "            mels_tensor = sample_batched['mel'].to(device) # [bs, mel_bins, L]\n",
    "            mel_length = sample_batched['mel_len'].to(device)\n",
    "\n",
    "            # Step 2. Run our forward pass\n",
    "\n",
    "            org_mel, pos_mel, stop_token, _ = model.forward(sample_batched)\n",
    "            loss1 = mse_loss(mels_tensor, org_mel)\n",
    "            loss1 += mse_loss(mels_tensor, pos_mel)\n",
    "\n",
    "            true_stop_token = torch.zeros(stop_token.shape).to(device)\n",
    "            for i in range(true_stop_token.shape[0]):\n",
    "                true_stop_token[i][mel_length[i]:]+=1.0\n",
    "            loss2 = bce_loss(torch.sigmoid(stop_token), true_stop_token)\n",
    "            \n",
    "            # Step 3. Run our backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss1 + loss2\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if loss.item()!=loss.item(): # if loss == NaN, break\n",
    "                print('NaN hit!')\n",
    "                exit()\n",
    "            \n",
    "            batch_train_loss.append(loss.item())\n",
    "\n",
    "            if i_batch % (3000 // BATCH_SIZE) == 0: # log about each n data\n",
    "                # test_loss = test()\n",
    "                test_loss = 0\n",
    "                train_loss = mean(batch_train_loss)\n",
    "                test_loss_q.append(test_loss)\n",
    "                train_loss_q.append(train_loss)\n",
    "                save_log(f'e{epoch}.txt', ['ğŸŸ£ epoch', epoch, 'data', i_batch*BATCH_SIZE, \n",
    "                    'lr', scheduler.get_last_lr(), \n",
    "                    'train_loss', '{:.3f}'.format(train_loss), \n",
    "                    'test_loss', test_loss, \n",
    "                    'bce_loss', '{:.3f}'.format(loss2.item())])\n",
    "                save_temp(epoch, test_loss) # save temp checkpoint\n",
    "                # test_decoder(epoch, 5)\n",
    "            \n",
    "            # exit()\n",
    "            \n",
    "        # scheduler.step()\n",
    "        save_checkpoint(epoch, mean(test_loss_q))\n",
    "        save_log(f'e{epoch}.txt', ['============= Final Test ============='])\n",
    "        # test_decoder(epoch, 10) # run some sample prediction and see the result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸŸ£ epoch 0 data 0 lr [0.001] train_loss 191.046 test_loss 0 bce_loss 1.031\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/scratch/bh2283/quartz_net/test.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000015vscode-remote?line=0'>1</a>\u001b[0m train()\n",
      "\u001b[1;32m/scratch/bh2283/quartz_net/test.ipynb Cell 14'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000014vscode-remote?line=9'>10</a>\u001b[0m mel_length \u001b[39m=\u001b[39m sample_batched[\u001b[39m'\u001b[39m\u001b[39mmel_len\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000014vscode-remote?line=11'>12</a>\u001b[0m \u001b[39m# Step 2. Run our forward pass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000014vscode-remote?line=13'>14</a>\u001b[0m org_mel, pos_mel, stop_token, _ \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(sample_batched)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000014vscode-remote?line=14'>15</a>\u001b[0m loss1 \u001b[39m=\u001b[39m mse_loss(mels_tensor, org_mel)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000014vscode-remote?line=15'>16</a>\u001b[0m loss1 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m mse_loss(mels_tensor, pos_mel)\n",
      "\u001b[1;32m/scratch/bh2283/quartz_net/test.ipynb Cell 7'\u001b[0m in \u001b[0;36mTacotronTail.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000006vscode-remote?line=41'>42</a>\u001b[0m \u001b[39m# print('mel_specgram', mel_specgram.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000006vscode-remote?line=42'>43</a>\u001b[0m mel_specgram_lengths \u001b[39m=\u001b[39m inputs[\u001b[39m'\u001b[39m\u001b[39mmel_len\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000006vscode-remote?line=43'>44</a>\u001b[0m mel_specgram, gate_outputs, alignments \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000006vscode-remote?line=44'>45</a>\u001b[0m     embedded_inputs, mel_specgram, memory_lengths\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mones_like(mel_specgram_lengths),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000006vscode-remote?line=45'>46</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000006vscode-remote?line=47'>48</a>\u001b[0m mel_specgram_postnet \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostnet(mel_specgram)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e5955485043227d/scratch/bh2283/quartz_net/test.ipynb#ch0000006vscode-remote?line=48'>49</a>\u001b[0m mel_specgram_postnet \u001b[39m=\u001b[39m mel_specgram \u001b[39m+\u001b[39m mel_specgram_postnet\n",
      "File \u001b[0;32m/scratch/bh2283/penv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/bh2283/penv/lib/python3.10/site-packages/torchaudio/models/tacotron2.py:741\u001b[0m, in \u001b[0;36m_Decoder.forward\u001b[0;34m(self, memory, mel_specgram_truth, memory_lengths, alpha)\u001b[0m\n\u001b[1;32m    729\u001b[0m decoder_input2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprenet(mel_output)\n\u001b[1;32m    730\u001b[0m decoder_input \u001b[39m=\u001b[39m alpha \u001b[39m*\u001b[39m decoder_input2 \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39malpha) \u001b[39m*\u001b[39m decoder_input\n\u001b[1;32m    731\u001b[0m (\n\u001b[1;32m    732\u001b[0m     mel_output,\n\u001b[1;32m    733\u001b[0m     gate_output,\n\u001b[1;32m    734\u001b[0m     attention_hidden,\n\u001b[1;32m    735\u001b[0m     attention_cell,\n\u001b[1;32m    736\u001b[0m     decoder_hidden,\n\u001b[1;32m    737\u001b[0m     decoder_cell,\n\u001b[1;32m    738\u001b[0m     attention_weights,\n\u001b[1;32m    739\u001b[0m     attention_weights_cum,\n\u001b[1;32m    740\u001b[0m     attention_context,\n\u001b[0;32m--> 741\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(\n\u001b[1;32m    742\u001b[0m     decoder_input,\n\u001b[1;32m    743\u001b[0m     attention_hidden,\n\u001b[1;32m    744\u001b[0m     attention_cell,\n\u001b[1;32m    745\u001b[0m     decoder_hidden,\n\u001b[1;32m    746\u001b[0m     decoder_cell,\n\u001b[1;32m    747\u001b[0m     attention_weights,\n\u001b[1;32m    748\u001b[0m     attention_weights_cum,\n\u001b[1;32m    749\u001b[0m     attention_context,\n\u001b[1;32m    750\u001b[0m     memory,\n\u001b[1;32m    751\u001b[0m     processed_memory,\n\u001b[1;32m    752\u001b[0m     mask,\n\u001b[1;32m    753\u001b[0m )\n\u001b[1;32m    755\u001b[0m mel_outputs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [mel_output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)]\n\u001b[1;32m    756\u001b[0m gate_outputs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [gate_output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)]\n",
      "File \u001b[0;32m/scratch/bh2283/penv/lib/python3.10/site-packages/torchaudio/models/tacotron2.py:658\u001b[0m, in \u001b[0;36m_Decoder.decode\u001b[0;34m(self, decoder_input, attention_hidden, attention_cell, decoder_hidden, decoder_cell, attention_weights, attention_weights_cum, attention_context, memory, processed_memory, mask)\u001b[0m\n\u001b[1;32m    656\u001b[0m \u001b[39m# attention_hiddenä¸ºLSTMçš„æœ€æ–°çš„hiddenï¼Œattention_cellä¹Ÿæ˜¯\u001b[39;00m\n\u001b[1;32m    657\u001b[0m attention_weights_cat \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((attention_weights\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), attention_weights_cum\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 658\u001b[0m attention_context, attention_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_layer(\n\u001b[1;32m    659\u001b[0m     attention_hidden, memory, processed_memory, attention_weights_cat, mask\n\u001b[1;32m    660\u001b[0m )\n\u001b[1;32m    661\u001b[0m \u001b[39m# attention_weightsæ˜¯æƒé‡ï¼Œattention_contextæ˜¯ä½¿ç”¨attention_weightsç»™memoryåŠ æƒåçš„å€¼\u001b[39;00m\n\u001b[1;32m    662\u001b[0m attention_weights_cum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m attention_weights\n",
      "File \u001b[0;32m/scratch/bh2283/penv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/bh2283/penv/lib/python3.10/site-packages/torchaudio/models/tacotron2.py:247\u001b[0m, in \u001b[0;36m_Attention.forward\u001b[0;34m(self, attention_hidden_state, memory, processed_memory, attention_weights_cat, mask)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    225\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    226\u001b[0m     attention_hidden_state: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     mask: Tensor,\n\u001b[1;32m    231\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[1;32m    232\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Pass the input through the Attention model.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \n\u001b[1;32m    234\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[39m        attention_weights (Tensor): Attention weights with shape (n_batch, max of ``text_lengths``).\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     alignment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_alignment_energies(attention_hidden_state, processed_memory, attention_weights_cat)\n\u001b[1;32m    249\u001b[0m     alignment \u001b[39m=\u001b[39m alignment\u001b[39m.\u001b[39mmasked_fill(mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscore_mask_value)\n\u001b[1;32m    251\u001b[0m     attention_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(alignment, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# è¾“å‡ºçš„energiesç®—æˆæ¦‚ç‡\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/bh2283/penv/lib/python3.10/site-packages/torchaudio/models/tacotron2.py:217\u001b[0m, in \u001b[0;36m_Attention._get_alignment_energies\u001b[0;34m(self, query, processed_memory, attention_weights_cat)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_alignment_energies\u001b[39m(\u001b[39mself\u001b[39m, query: Tensor, processed_memory: Tensor, attention_weights_cat: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    204\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Get the alignment vector.\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \n\u001b[1;32m    206\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39m        alignment (Tensor): attention weights, it is a tensor with shape (batch, max of ``text_lengths``).\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     processed_query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquery_layer(query\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m1\u001b[39;49m))\n\u001b[1;32m    218\u001b[0m     processed_attention_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlocation_layer(attention_weights_cat) \u001b[39m# è¿™ä¸ªæ˜¯ä½ç½®ç¼–ç \u001b[39;00m\n\u001b[1;32m    219\u001b[0m     energies \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv(torch\u001b[39m.\u001b[39mtanh(processed_query \u001b[39m+\u001b[39m processed_attention_weights \u001b[39m+\u001b[39m processed_memory))\n",
      "File \u001b[0;32m/scratch/bh2283/penv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/scratch/bh2283/penv/lib/python3.10/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3322949f56cb4db99427e05ed2d4a87f0497ffa3e41dd81b99d577253bd3be5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
