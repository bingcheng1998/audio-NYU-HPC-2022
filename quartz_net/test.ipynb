{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import OpencpopDataset, MusicLoaderGenerator\n",
    "from helper import parser_line, merge_note, get_pitch_labels, get_transposed_phoneme_labels, print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_transform(sample, sample_rate=None):\n",
    "    id, text, phoneme, note, note_duration, phoneme_duration, slur_note = parser_line(sample['text'])\n",
    "    text_with_p, phoneme, note, note_duration, slur_note = merge_note(text, phoneme, note, note_duration, slur_note)\n",
    "    sample['chinese'] = text_with_p\n",
    "    sample['phoneme'] = phoneme\n",
    "    sample['note'] = note\n",
    "    sample['duration'] = note_duration\n",
    "    sample['slur'] = slur_note\n",
    "    return sample\n",
    "\n",
    "dataset = OpencpopDataset('/scratch/bh2283/data/opencpop/segments/', transform=dataset_transform, sample_rate=22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3744, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = dataset.split()\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: 3744 test_set: 12\n",
      "['你', '说', '空', '瓶', '适', '合', '许', 'SP', '愿', 'SP', 'AP', '根', '本', '不', '重', '要', 'SP']\n",
      "torch.Size([17, 2])\n",
      "torch.Size([17, 2])\n",
      "torch.Size([17])\n",
      "tensor([12, 17, 18, 14, 17, 14, 33,  4, 42, 82, 15, 15, 12, 15, 28, 32, 98])\n",
      "torch.Size([17, 80, 225])\n",
      "torch.Size([17]) tensor([ 26,  36,  38,  29,  36,  29,  67,  10,  86, 166,  32,  32,  25,  31,\n",
      "         58,  65, 225])\n"
     ]
    }
   ],
   "source": [
    "note_labels = get_pitch_labels()\n",
    "phoneme_labels = get_transposed_phoneme_labels()\n",
    "slur_labels = [-1, 0, 1]\n",
    "# 0-1 分辨率0.01，1-2 分辨率0.05，2-7 分辨率0.2\n",
    "duration_labels = [i for i in range(7)]\n",
    "\n",
    "labels = (\n",
    "    phoneme_labels,\n",
    "    note_labels,\n",
    "    slur_labels\n",
    ")\n",
    "loaderGenerator = MusicLoaderGenerator(labels)\n",
    "train_loader = loaderGenerator.dataloader(train_set, batch_size=2)\n",
    "print('train_set:', len(train_set), 'test_set:',len(test_set))\n",
    "steps = 1\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    if steps <= 0:\n",
    "        break\n",
    "\n",
    "    print(sample_batched['chinese'])\n",
    "    print(sample_batched['phoneme'].shape)\n",
    "    print(sample_batched['phoneme_pre'].shape)\n",
    "    print(sample_batched['note_post'].shape)\n",
    "    print(sample_batched['audio_duration_quant'])\n",
    "    print(sample_batched['mel'].shape)\n",
    "    print(sample_batched['mel_len'].shape, sample_batched['mel_len'])\n",
    "    steps -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 设计\n",
    "\n",
    "- 尝试使用逆卷积，上采样得到所需的音频\n",
    "- 我们无需去计算时间停止符，只需要在输出的时间内计算loss并且最小化即可\n",
    "- 设定一个最大时间长度，比如两秒，超过的就不要了（用阈值筛掉）\n",
    "- 多层上采样得到最佳的输出\n",
    "- 使用梅尔频谱，还有解码器，可以使得输出音质比stft好（猜的，需要验证）一般机器学习声码器都会好点\n",
    "\n",
    "但是有问题：\n",
    "- 使用逆卷积太过刚直，没有变化性，导致无法很好的收敛\n",
    "- 一般逆卷积和GAN一起使用，用判别器取代刚直的loss\n",
    "- 使用tacotron模式就会好很多，无需GAN，自收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.tacotron2 import Tacotron2, _get_mask_from_lengths, _Decoder, _Encoder, _Postnet\n",
    "from torchaudio.pipelines._tts.utils import _get_taco_params\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from typing import Tuple, List, Optional, Union, overload\n",
    "\n",
    "class TacotronTail(Tacotron2):\n",
    "    def __init__(\n",
    "        self,\n",
    "        labels_lens: dict,\n",
    "        decoder = None,\n",
    "        postnet = None,\n",
    "    ) -> None:\n",
    "        _tacotron2_params=_get_taco_params(n_symbols=5) # ignore n_symbols, encoder not used \n",
    "        super().__init__(**_tacotron2_params)\n",
    "\n",
    "        embedding_dim = _tacotron2_params['encoder_embedding_dim']\n",
    "        self.embeddings = {\n",
    "            key: torch.nn.Embedding(value, embedding_dim)\n",
    "            for key, value in labels_lens.items()\n",
    "        }\n",
    "        self.embedding_register = torch.nn.ModuleList(self.embeddings.values())\n",
    "        # 将embedding注册进模型，不确定是否复制，需要在实践中测试\n",
    "        if decoder is not None:\n",
    "            self.decoder: _Decoder = decoder\n",
    "        if postnet is not None:\n",
    "            self.postnet: _Postnet = postnet\n",
    "        self.reduce_phoneme = lambda x: torch.sum(x, 1) if len(x.shape)==3 else x\n",
    "        self.decoder.decoder_max_step = int(4 * 22050 / 256)\n",
    "        self.version = '0.01'\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: dict,\n",
    "    ):\n",
    "        embedded_inputs = [\n",
    "            self.reduce_phoneme(self.embeddings[key](inputs[key])) for key in self.embeddings.keys()\n",
    "        ]\n",
    "        embedded_inputs = torch.stack(embedded_inputs).sum(0).unsqueeze(1) # [bs, 1, emb_size]\n",
    "        print('embedded_inputs', embedded_inputs.shape)\n",
    "        mel_specgram = inputs['mel'] # (n_batch, ``n_mels``, max of ``mel_specgram_lengths``)\n",
    "        print('mel_specgram', mel_specgram.shape)\n",
    "        mel_specgram_lengths = inputs['mel_len']\n",
    "        mel_specgram, gate_outputs, alignments = self.decoder(\n",
    "            embedded_inputs, mel_specgram, memory_lengths=torch.ones_like(mel_specgram_lengths),\n",
    "        )\n",
    "\n",
    "        mel_specgram_postnet = self.postnet(mel_specgram)\n",
    "        mel_specgram_postnet = mel_specgram + mel_specgram_postnet\n",
    "\n",
    "        if self.mask_padding:\n",
    "            mask = _get_mask_from_lengths(mel_specgram_lengths)\n",
    "            mask = mask.expand(self.n_mels, mask.size(0), mask.size(1))\n",
    "            mask = mask.permute(1, 0, 2)\n",
    "\n",
    "            mel_specgram.masked_fill_(mask, 0.0)\n",
    "            mel_specgram_postnet.masked_fill_(mask, 0.0)\n",
    "            gate_outputs.masked_fill_(mask[:, 0, :], 1e3)\n",
    "\n",
    "        return mel_specgram, mel_specgram_postnet, gate_outputs, alignments\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(\n",
    "        self, \n",
    "        inputs: dict,\n",
    "        ) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "\n",
    "        embedded_inputs = [\n",
    "            self.reduce_phoneme(self.embeddings[key](inputs[key])) for key in self.embeddings.keys()\n",
    "        ]\n",
    "        embedded_inputs = torch.stack(embedded_inputs).sum(0).unsqueeze(1) # [bs, 1, emb_size]\n",
    "        print('embedded_inputs', embedded_inputs.shape)\n",
    "        \n",
    "        n_batch = embedded_inputs.shape[0]\n",
    "        mel_specgram, mel_specgram_lengths, _, alignments = \\\n",
    "            self.decoder.infer(embedded_inputs, memory_lengths=torch.ones(n_batch))\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_specgram)\n",
    "        mel_outputs_postnet = mel_specgram + mel_outputs_postnet\n",
    "\n",
    "        alignments = alignments.unfold(1, n_batch, n_batch).transpose(0, 2)\n",
    "\n",
    "        return mel_outputs_postnet, mel_specgram_lengths, alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedded_inputs torch.Size([25, 1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/bh2283/penv/lib/python3.10/site-packages/torchaudio/models/tacotron2.py:860: UserWarning: Reached max decoder steps. The generated spectrogram might not cover the whole transcript.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "labels_lens = {\n",
    "    'audio_duration_quant': 130, # 这个是量化后的计算结果\n",
    "    'phoneme': len(phoneme_labels), # 拼音\n",
    "    'phoneme_pre': len(phoneme_labels), # 前一个汉字的拼音\n",
    "    'phoneme_post': len(phoneme_labels), # 后一个汉字的拼音\n",
    "    'note': len(note_labels), # 音调音符\n",
    "    'note_pre': len(note_labels),\n",
    "    'note_post': len(note_labels),\n",
    "    'slur': len(slur_labels), # 是否为延长音\n",
    "}\n",
    "model = TacotronTail(labels_lens)\n",
    "steps = 1\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    if steps <= 0:\n",
    "        break\n",
    "    model.infer(sample_batched)\n",
    "    # print(sample_batched['chinese'])\n",
    "    # print(sample_batched['phoneme'].shape)\n",
    "    # print(sample_batched['phoneme_pre'].shape)\n",
    "    # print(sample_batched['note_post'].shape)\n",
    "    # print(sample_batched['audio_duration_quant'])\n",
    "    # print(sample_batched['mel'].shape)\n",
    "    # print(sample_batched['mel_len'].shape, sample_batched['mel_len'])\n",
    "    steps -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3322949f56cb4db99427e05ed2d4a87f0497ffa3e41dd81b99d577253bd3be5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
