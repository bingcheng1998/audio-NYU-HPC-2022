{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import OpencpopDataset, MusicLoaderGenerator\n",
    "from helper import parser_line, merge_note, get_pitch_labels, get_transposed_phoneme_labels, print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_transform(sample, sample_rate=None):\n",
    "    id, text, phoneme, note, note_duration, phoneme_duration, slur_note = parser_line(sample['text'])\n",
    "    text_with_p, phoneme, note, note_duration, slur_note = merge_note(text, phoneme, note, note_duration, slur_note)\n",
    "    sample['chinese'] = text_with_p\n",
    "    sample['phoneme'] = phoneme\n",
    "    sample['note'] = note\n",
    "    sample['duration'] = note_duration\n",
    "    sample['slur'] = slur_note\n",
    "    return sample\n",
    "\n",
    "dataset = OpencpopDataset('/scratch/bh2283/data/opencpop/segments/', transform=dataset_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3744, 12)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = dataset.split()\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set: 3744 test_set: 12\n",
      "dict_keys(['audio', 'audio_len', 'audio_duration', 'audio_duration_quant', 'chinese', 'phoneme', 'phoneme_pre', 'phoneme_post', 'note', 'note_pre', 'note_post', 'slur'])\n",
      "['愿', '我', '如', '烟', 'AP', '还', '愿', '我', '曼', '丽', '又', '懒', '倦', '我', 'SP', '等', '的', '人', 'SP', '他', '在', 'SP', '多', '远', '的', '未', '来', 'SP', 'AP', 'SP']\n",
      "[[23, 45], [24, 31], [19, 26], [23, 42], [60, 0], [12, 36], [23, 45], [24, 31], [4, 42], [9, 25], [23, 41], [9, 42], [13, 45], [24, 31], [61, 0], [6, 52], [6, 33], [19, 46], [61, 0], [7, 28], [20, 36], [61, 0], [6, 32], [23, 45], [6, 33], [24, 38], [9, 36], [61, 0], [60, 0], [61, 0]]\n"
     ]
    }
   ],
   "source": [
    "note_labels = get_pitch_labels()\n",
    "phoneme_labels = get_transposed_phoneme_labels()\n",
    "slur_labels = [0, 1]\n",
    "# 0-1 分辨率0.01，1-2 分辨率0.05，2-7 分辨率0.2\n",
    "duration_labels = [i for i in range(7)]\n",
    "\n",
    "labels = (\n",
    "    phoneme_labels,\n",
    "    note_labels,\n",
    "    duration_labels,\n",
    "    slur_labels\n",
    ")\n",
    "loaderGenerator = MusicLoaderGenerator(labels)\n",
    "train_loader = loaderGenerator.dataloader(train_set, batch_size=2)\n",
    "print('train_set:', len(train_set), 'test_set:',len(test_set))\n",
    "steps = 1\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    if steps <= 0:\n",
    "        break\n",
    "\n",
    "    print(sample_batched.keys())\n",
    "    print(sample_batched['chinese'])\n",
    "    print(sample_batched['phoneme'])\n",
    "    # print_all(sample_batched.values())\n",
    "    steps -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 设计\n",
    "\n",
    "- 尝试使用逆卷积，上采样得到所需的音频\n",
    "- 我们无需去计算时间停止符，只需要在输出的时间内计算loss并且最小化即可\n",
    "- 设定一个最大时间长度，比如两秒，超过的就不要了（用阈值筛掉）\n",
    "- 多层上采样得到最佳的输出\n",
    "- 使用梅尔频谱，还有解码器，可以使得输出音质比stft好（猜的，需要验证）一般机器学习声码器都会好点\n",
    "\n",
    "但是有问题：\n",
    "- 使用逆卷积太过刚直，没有变化性，导致无法很好的收敛\n",
    "- 一般逆卷积和GAN一起使用，用判别器取代刚直的loss\n",
    "- 使用tacotron模式就会好很多，无需GAN，自收敛"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.models.tacotron2 import Tacotron2\n",
    "class TacotronTail(Tacotron2):\n",
    "    def __init__(\n",
    "        self,\n",
    "        labels,\n",
    "        speaker_emb_size = 128,\n",
    "        decoder = None,\n",
    "        postnet = None,\n",
    "    ) -> None:\n",
    "        _tacotron2_params=_get_taco_params(n_symbols=len(labels))\n",
    "        super().__init__(**_tacotron2_params)\n",
    "\n",
    "        cutted_encoder_embedding_dim = _tacotron2_params['encoder_embedding_dim'] - speaker_emb_size\n",
    "        self.embedding = torch.nn.Embedding(_tacotron2_params['n_symbol'], cutted_encoder_embedding_dim)\n",
    "        self.encoder: _Encoder = _Encoder(cutted_encoder_embedding_dim, _tacotron2_params['encoder_n_convolution'], _tacotron2_params['encoder_kernel_size'])\n",
    "        if decoder is not None:\n",
    "            self.decoder: _Decoder = decoder\n",
    "        if postnet is not None:\n",
    "            self.postnet: _Postnet = postnet\n",
    "        self.speaker_emb_size = speaker_emb_size\n",
    "        self.version = '0.05'\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        tokens: Tensor,\n",
    "        token_lengths: Tensor,\n",
    "        mel_specgram: Tensor,\n",
    "        mel_specgram_lengths: Tensor,\n",
    "        speaker_emb: Optional[Tensor]=None,\n",
    "        alpha: int = 0,\n",
    "    ):\n",
    "        embedded_inputs = self.embedding(tokens).transpose(1, 2) \n",
    "\n",
    "        mel_specgram, gate_outputs, alignments = self.decoder(\n",
    "            encoder_outputs, mel_specgram, memory_lengths=token_lengths, alpha=alpha\n",
    "        )\n",
    "\n",
    "        mel_specgram_postnet = self.postnet(mel_specgram)\n",
    "        mel_specgram_postnet = mel_specgram + mel_specgram_postnet\n",
    "\n",
    "        if self.mask_padding:\n",
    "            mask = _get_mask_from_lengths(mel_specgram_lengths)\n",
    "            mask = mask.expand(self.n_mels, mask.size(0), mask.size(1))\n",
    "            mask = mask.permute(1, 0, 2)\n",
    "\n",
    "            mel_specgram.masked_fill_(mask, 0.0)\n",
    "            mel_specgram_postnet.masked_fill_(mask, 0.0)\n",
    "            gate_outputs.masked_fill_(mask[:, 0, :], 1e3)\n",
    "\n",
    "        return mel_specgram, mel_specgram_postnet, gate_outputs, alignments\n",
    "\n",
    "    @torch.jit.export\n",
    "    def infer(self, tokens: Tensor, lengths: Optional[Tensor] = None, speaker_emb: Optional[Tensor]=None) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        n_batch, max_length = tokens.shape\n",
    "        if lengths is None:\n",
    "            lengths = torch.tensor([max_length]).expand(n_batch).to(tokens.device, tokens.dtype)\n",
    "\n",
    "        assert lengths is not None  # For TorchScript compiler\n",
    "\n",
    "        embedded_inputs = self.embedding(tokens).transpose(1, 2)\n",
    "        encoder_outputs = self.encoder(embedded_inputs, lengths)\n",
    "        \n",
    "        mel_specgram, mel_specgram_lengths, _, alignments = self.decoder.infer(encoder_outputs, lengths)\n",
    "\n",
    "        mel_outputs_postnet = self.postnet(mel_specgram)\n",
    "        mel_outputs_postnet = mel_specgram + mel_outputs_postnet\n",
    "\n",
    "        alignments = alignments.unfold(1, n_batch, n_batch).transpose(0, 2)\n",
    "\n",
    "        return mel_outputs_postnet, mel_specgram_lengths, alignments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3322949f56cb4db99427e05ed2d4a87f0497ffa3e41dd81b99d577253bd3be5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
