{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_initial_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语音数据集预处理\n",
    "\n",
    "- 将连续的多音节重叠到一起，划分单个汉字的时间\n",
    "- 延长发音需要标注出来，确定为延长，方便之后生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_table = get_initial_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all(x): \n",
    "    for s in x:\n",
    "        print(len(s), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcriptions(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        if (lines[-1]==''):\n",
    "            lines = lines[:-1]\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3550"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../data/opencpop/segments/'\n",
    "lines = get_transcriptions(path+'train.txt')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(id, path, sr = 16000):\n",
    "    wav_path = path+'wavs/'+str(id)+'.wav'\n",
    "    waveform, sample_rate = torchaudio.load(wav_path)\n",
    "    if sample_rate != sr:\n",
    "        waveform = torchaudio.functional.resample(waveform[0].unsqueeze(0), sample_rate, sr)\n",
    "    return waveform\n",
    "\n",
    "def parser_line(line):\n",
    "    id, text, phoneme, note, note_duration, phoneme_duration, slur_note = line.split('|')\n",
    "    phoneme = phoneme.split(' ')\n",
    "    note = note.split(' ')\n",
    "    note_duration = [float(i) for i in note_duration.split(' ')]\n",
    "    phoneme_duration = [float(i) for i in phoneme_duration.split(' ')]\n",
    "    slur_note = [int(i) for i in slur_note.split(' ')]\n",
    "    assert len(phoneme) == len(note_duration) and len(phoneme_duration) == len(slur_note) and len(slur_note) == len(phoneme)\n",
    "    return id, text, phoneme, note, note_duration, phoneme_duration, slur_note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看下一共用到了多少元音辅音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_set = set()\n",
    "note_set = set()\n",
    "for line in lines:\n",
    "    id, text, phoneme, note, note_duration, phoneme_duration, slur_note = parser_line(line)\n",
    "    phoneme_set.update(set(phoneme))\n",
    "    note_set.update(set(note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 {'uai', 'iang', 'd', 'uo', 'sh', 'h', 'ao', 'e', 'z', 'r', 'ie', 'zh', 'w', 'x', 'an', 'u', 'o', 'vn', 'in', 'SP', 'en', 'iu', 'ian', 'f', 'ing', 'i', 'ei', 'iong', 'ou', 'a', 'ui', 'AP', 't', 'p', 'uan', 'eng', 'ang', 'iao', 'g', 'l', 'ong', 'ch', 'j', 'van', 'm', 'un', 's', 'v', 'k', 'y', 'c', 'er', 've', 'ai', 'q', 'ia', 'n', 'ua', 'uang', 'b'}\n",
      "35 {'D#4/Eb4', 'A3', 'G#4/Ab4', 'G4', 'B3', 'B4', 'D4', 'A#3/Bb3', 'D5', 'D#3/Eb3', 'E3', 'D2', 'A5', 'F5', 'rest', 'F#4/Gb4', 'C4', 'G3', 'G#3/Ab3', 'F3', 'A4', 'C#2/Db2', 'D3', 'E4', 'A#4/Bb4', 'D#5/Eb5', 'F4', 'C#3/Db3', 'F#3/Gb3', 'C#4/Db4', 'C5', 'C3', 'F#5/Gb5', 'C#5/Db5', 'E5'}\n"
     ]
    }
   ],
   "source": [
    "print_all([phoneme_set,note_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打一个示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2001000006|漂浮在一片无奈|p iao f u z ai ai ai AP SP y i i p ian ian ian w u n ai SP AP|E4 E4 F#4/Gb4 F#4/Gb4 G#4/Ab4 G#4/Ab4 A4 G#4/Ab4 rest rest E4 E4 F#4/Gb4 G#4/Ab4 G#4/Ab4 A4 G#4/Ab4 E4 E4 F#4/Gb4 F#4/Gb4 rest rest|0.185230 0.185230 0.177410 0.177410 0.193930 0.193930 0.259670 0.299340 0.215550 0.031770 0.197520 0.197520 0.165450 0.184760 0.184760 0.212290 0.246960 0.440370 0.440370 1.524950 1.524950 0.855830 0.559100|0.06011 0.12512 0.07517 0.10224 0.08603 0.1079 0.25967 0.29934 0.21555 0.03177 0.05175 0.14577 0.16545 0.0748 0.10996 0.21229 0.24696 0.09617 0.3442 0.1437 1.38125 0.85583 0.5591|0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = lines[5]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2001000006\n",
      "7 漂浮在一片无奈\n",
      "23 ['p', 'iao', 'f', 'u', 'z', 'ai', 'ai', 'ai', 'AP', 'SP', 'y', 'i', 'i', 'p', 'ian', 'ian', 'ian', 'w', 'u', 'n', 'ai', 'SP', 'AP']\n",
      "23 ['E4', 'E4', 'F#4/Gb4', 'F#4/Gb4', 'G#4/Ab4', 'G#4/Ab4', 'A4', 'G#4/Ab4', 'rest', 'rest', 'E4', 'E4', 'F#4/Gb4', 'G#4/Ab4', 'G#4/Ab4', 'A4', 'G#4/Ab4', 'E4', 'E4', 'F#4/Gb4', 'F#4/Gb4', 'rest', 'rest']\n",
      "23 [0.18523, 0.18523, 0.17741, 0.17741, 0.19393, 0.19393, 0.25967, 0.29934, 0.21555, 0.03177, 0.19752, 0.19752, 0.16545, 0.18476, 0.18476, 0.21229, 0.24696, 0.44037, 0.44037, 1.52495, 1.52495, 0.85583, 0.5591]\n",
      "23 [0.06011, 0.12512, 0.07517, 0.10224, 0.08603, 0.1079, 0.25967, 0.29934, 0.21555, 0.03177, 0.05175, 0.14577, 0.16545, 0.0748, 0.10996, 0.21229, 0.24696, 0.09617, 0.3442, 0.1437, 1.38125, 0.85583, 0.5591]\n",
      "23 [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "id, text, phoneme, note, note_duration, phoneme_duration, slur_note = parser_line(line)\n",
    "print_all([id, text, phoneme, note, note_duration, phoneme_duration, slur_note])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "音频文件读取测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 92003])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform = get_audio(id, path)\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "汉字元音辅音组合为单个汉字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_note(text, phoneme, note, note_duration):\n",
    "    # remove the duplicate items in phoneme, note, and note_duration\n",
    "    # use text to verify the length\n",
    "    phoneme = phoneme.copy()\n",
    "    note = note.copy()\n",
    "    note_duration = note_duration.copy()\n",
    "    j = -1\n",
    "    text+='////////////////////'\n",
    "    text_with_p = phoneme.copy()\n",
    "    used_flag = False\n",
    "    for i in range(len(text_with_p)):\n",
    "        if text_with_p[i] in ['AP', 'SP']:\n",
    "            continue\n",
    "        if j==-1 or phoneme[i] in initial_table or (phoneme[i-1] not in initial_table and phoneme[i] != phoneme[i-1]):\n",
    "            j+=1\n",
    "            used_flag = False\n",
    "        text_with_p[i] = text[j] if used_flag == False else '~'\n",
    "        used_flag = True\n",
    "    for i in range(len(phoneme)-1, 0, -1):\n",
    "        if (note_duration[i] == note_duration[i-1] and phoneme[i-1] in initial_table):\n",
    "            del note_duration[i]\n",
    "            del note[i]\n",
    "            phoneme[i-1]=phoneme[i-1]+phoneme[i]\n",
    "            del phoneme[i]\n",
    "            del text_with_p[i]\n",
    "    return text_with_p, phoneme, note, note_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_with_p, phoneme, note, note_duration = merge_note(text, phoneme, note, note_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 ['漂', '浮', '在', '~', '~', 'AP', 'SP', '一', '~', '片', '~', '~', '无', '奈', 'SP', 'AP']\n",
      "16 ['piao', 'fu', 'zai', 'ai', 'ai', 'AP', 'SP', 'yi', 'i', 'pian', 'ian', 'ian', 'wu', 'nai', 'SP', 'AP']\n",
      "16 ['E4', 'F#4/Gb4', 'G#4/Ab4', 'A4', 'G#4/Ab4', 'rest', 'rest', 'E4', 'F#4/Gb4', 'G#4/Ab4', 'A4', 'G#4/Ab4', 'E4', 'F#4/Gb4', 'rest', 'rest']\n",
      "16 [0.18523, 0.17741, 0.19393, 0.25967, 0.29934, 0.21555, 0.03177, 0.19752, 0.16545, 0.18476, 0.21229, 0.24696, 0.44037, 1.52495, 0.85583, 0.5591]\n"
     ]
    }
   ],
   "source": [
    "print_all([text_with_p, phoneme, note, note_duration])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SpeechDataset\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpencpopDataset(SpeechDataset):\n",
    "\n",
    "    def __init__(self, data_path, sample_rate=16000, transform=None):\n",
    "        super().__init__(data_path, sample_rate, transform)\n",
    "        transcript_file = data_path+'transcriptions.txt'\n",
    "        self.transcript = self.gen_transcript(transcript_file)\n",
    "        self.dataset_file_num = len(self.transcript)\n",
    "        self.threshold = 120000 # to avoid GPU memory used out\n",
    "        self.batch_size = 80 # to avoid GPU memory used out\n",
    "        self.split_ratio = [1000, 3]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_file_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if idx >= self.dataset_file_num:\n",
    "            return {'audio': None, 'text': None}\n",
    "        line = self.transcript[idx]\n",
    "        id, text, phoneme, note, note_duration, phoneme_duration, slur_note = self.parser_line(line)\n",
    "        waveform = self.get_audio(id)\n",
    "        # text_with_p, phoneme, note, note_duration = merge_note(text, phoneme, note, note_duration)\n",
    "        sample = {'audio': waveform, 'text': line}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample, self.sample_rate)\n",
    "        return sample\n",
    "\n",
    "    def get_audio(self, id):\n",
    "        wav_path = self.data_path+'wavs/'+str(id)+'.wav'\n",
    "        waveform, sample_rate = torchaudio.load(wav_path)\n",
    "        if sample_rate != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform[0].unsqueeze(0), sample_rate, self.sample_rate)\n",
    "        return waveform\n",
    "\n",
    "    def parser_line(self, line):\n",
    "        id, text, phoneme, note, note_duration, phoneme_duration, slur_note = line.split('|')\n",
    "        phoneme = phoneme.split(' ')\n",
    "        note = note.split(' ')\n",
    "        note_duration = [float(i) for i in note_duration.split(' ')]\n",
    "        phoneme_duration = [float(i) for i in phoneme_duration.split(' ')]\n",
    "        slur_note = [int(i) for i in slur_note.split(' ')]\n",
    "        assert len(phoneme) == len(note_duration) and len(phoneme_duration) == len(slur_note) and len(slur_note) == len(phoneme)\n",
    "        return id, text, phoneme, note, note_duration, phoneme_duration, slur_note\n",
    "\n",
    "    def gen_transcript(self, transcript_file):\n",
    "        with open(transcript_file) as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            if (lines[-1]==''):\n",
    "                lines = lines[:-1]\n",
    "            return lines\n",
    "\n",
    "    def split(self, split_ratio=None, seed=42):\n",
    "        audio_dataset = self\n",
    "        size = len(audio_dataset)\n",
    "        my_split_ratio = self.split_ratio if split_ratio is None else split_ratio\n",
    "        lengths = [(i*size)//sum(my_split_ratio) for i in my_split_ratio]\n",
    "        lengths[-1] = size - sum(lengths[:-1])\n",
    "        split_dataset = random_split(audio_dataset, lengths, generator=torch.Generator().manual_seed(seed))\n",
    "        return split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataset_transform(sample, sample_rate=None):\n",
    "    id, text, phoneme, note, note_duration, phoneme_duration, slur_note = parser_line(sample['text'])\n",
    "    text_with_p, phoneme, note, note_duration = merge_note(text, phoneme, note, note_duration)\n",
    "    sample['chinese'] = text_with_p\n",
    "    sample['phoneme'] = phoneme\n",
    "    sample['note'] = note\n",
    "    sample['duration'] = note_duration\n",
    "    return sample\n",
    "\n",
    "dataset = OpencpopDataset('/scratch/bh2283/data/opencpop/segments/', transform=dataset_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor([[ 0.0005,  0.0006,  0.0004,  ..., -0.0026, -0.0027, -0.0024]])\n",
      "434 2001000018|点燃所有希望|d ian r an AP s ou y ou x i w ang ang SP AP|B3 B3 G#4/Ab4 G#4/Ab4 rest B3 B3 B3 B3 B3 B3 E4 E4 F#4/Gb4 rest rest|0.604600 0.604600 0.942130 0.942130 0.343040 0.420960 0.420960 0.260520 0.260520 0.831380 0.831380 0.227950 0.227950 0.841250 0.832080 0.401940|0.03378 0.57082 0.09136 0.85077 0.34304 0.17748 0.24348 0.08291 0.17761 0.24236 0.58902 0.04003 0.18792 0.84125 0.83208 0.40194|0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      "10 ['点', '燃', 'AP', '所', '有', '希', '望', '~', 'SP', 'AP']\n",
      "10 ['dian', 'ran', 'AP', 'sou', 'you', 'xi', 'wang', 'ang', 'SP', 'AP']\n",
      "10 ['B3', 'G#4/Ab4', 'rest', 'B3', 'B3', 'B3', 'E4', 'F#4/Gb4', 'rest', 'rest']\n",
      "10 [0.6046, 0.94213, 0.34304, 0.42096, 0.26052, 0.83138, 0.22795, 0.84125, 0.83208, 0.40194]\n"
     ]
    }
   ],
   "source": [
    "print_all(dataset[17].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = dataset.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicLoaderGenerator:\n",
    "    def __init__(self, \n",
    "        labels, k_size=0, \n",
    "        num_workers=0,\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        ) -> None:\n",
    "        self.k_size = k_size\n",
    "        self.labels = labels\n",
    "        self.look_up = {s: i for i, s in enumerate(labels)}\n",
    "        self.device = device\n",
    "        self.num_workers = num_workers\n",
    "        self.version = '0.02'\n",
    "\n",
    "    def label2id(self, str):\n",
    "        return [self.look_up[i] for i in str]\n",
    "\n",
    "    def id2label(self, idcs):\n",
    "        return ''.join([self.labels[i] for i in idcs])\n",
    "\n",
    "    def batch_filter(self, batch:list):\n",
    "        # remove all audio with tag if audio length > threshold\n",
    "        for i in range(len(batch)-1, -1, -1):\n",
    "            if batch[i]['audio'].shape[-1] > self.threshold:\n",
    "                del batch[i]\n",
    "        return batch\n",
    "\n",
    "    def collate_wrapper(self, batch:list): # RAW\n",
    "        batch = self.batch_filter(batch)\n",
    "        bs = len(batch)\n",
    "        rand_shift = torch.randint(self.k_size, (bs,))\n",
    "        audio_list = [batch[i]['audio'][:,rand_shift[i]:] for i in range(bs)]\n",
    "        audio_length = [audio.shape[-1] for audio in audio_list]\n",
    "        target_list = [self.label2id(item['text']) for item in batch]\n",
    "        target_length = [len(l) for l in target_list]\n",
    "        chinese_list = [batch[i]['chinese'] for i in range(bs)]\n",
    "\n",
    "        target_length, target_list, audio_length, audio_list, chinese_list = zip(*sorted(zip(target_length, target_list, audio_length, audio_list, chinese_list), reverse=True))\n",
    "        target_length = torch.tensor(target_length)\n",
    "        audio_length = torch.tensor(audio_length)\n",
    "\n",
    "        max_audio_length = torch.max(audio_length)\n",
    "        audio_list = torch.cat([\n",
    "            torch.cat(\n",
    "            (audio, torch.zeros(max_audio_length-audio.shape[-1]).unsqueeze(0)), -1)\n",
    "            for audio in audio_list], 0)\n",
    "        \n",
    "        max_target_length = torch.max(target_length)\n",
    "        target_list = torch.cat([\n",
    "            torch.cat(\n",
    "            (torch.tensor(l), torch.zeros([max_target_length-len(l)], dtype=torch.int)), -1).unsqueeze(0) \n",
    "            for l in target_list], 0)\n",
    "        return {'audio': audio_list, 'audio_len': audio_length, \n",
    "                'target': target_list, 'target_len': target_length,\n",
    "                'chinese': chinese_list}\n",
    "\n",
    "    def dataloader(self, audioDataset, batch_size, shuffle=True):\n",
    "        # k_size is the kernel size for the encoder, for data augmentation\n",
    "        self.threshold = audioDataset.dataset.threshold\n",
    "        return DataLoader(audioDataset, batch_size,\n",
    "                            shuffle, num_workers=self.num_workers, collate_fn=self.collate_wrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaderGenerator = MusicLoaderGenerator(labels, k_size=5)\n",
    "train_loader = loaderGenerator.dataloader(train_set, batch_size=8)\n",
    "print('train_set:', len(train_set), 'test_set:',len(test_set))\n",
    "    steps = 10\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        if steps <= 0:\n",
    "            break\n",
    "        print(sample_batched['audio'].shape, sample_batched['target'].shape)\n",
    "        print(sample_batched['audio_len'], sample_batched['target_len'])\n",
    "        steps -= 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3322949f56cb4db99427e05ed2d4a87f0497ffa3e41dd81b99d577253bd3be5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
