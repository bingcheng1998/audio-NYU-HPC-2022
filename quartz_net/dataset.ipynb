{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import get_initial_table, get_final_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语音数据集预处理\n",
    "\n",
    "- 将连续的多音节重叠到一起，划分单个汉字的时间\n",
    "- 延长发音需要标注出来，确定为延长，方便之后生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_table = get_initial_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_all(x): \n",
    "    for s in x:\n",
    "        print(len(s), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcriptions(path):\n",
    "    with open(path) as f:\n",
    "        lines = f.read().split('\\n')\n",
    "        if (lines[-1]==''):\n",
    "            lines = lines[:-1]\n",
    "        return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3550"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../data/opencpop/segments/'\n",
    "lines = get_transcriptions(path+'train.txt')\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio(id, path, sr = 16000):\n",
    "    wav_path = path+'wavs/'+str(id)+'.wav'\n",
    "    waveform, sample_rate = torchaudio.load(wav_path)\n",
    "    if sample_rate != sr:\n",
    "        waveform = torchaudio.functional.resample(waveform[0].unsqueeze(0), sample_rate, sr)\n",
    "    return waveform\n",
    "\n",
    "def parser_line(line):\n",
    "    id, text, phoneme, note, note_duration, phoneme_duration, slur_note = line.split('|')\n",
    "    phoneme = phoneme.split(' ')\n",
    "    note = note.split(' ')\n",
    "    note_duration = [float(i) for i in note_duration.split(' ')]\n",
    "    phoneme_duration = [float(i) for i in phoneme_duration.split(' ')]\n",
    "    slur_note = [int(i) for i in slur_note.split(' ')]\n",
    "    assert len(phoneme) == len(note_duration) and len(phoneme_duration) == len(slur_note) and len(slur_note) == len(phoneme)\n",
    "    return id, text, phoneme, note, note_duration, phoneme_duration, slur_note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看下一共用到了多少元音辅音"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "phoneme_set = set()\n",
    "note_set = set()\n",
    "for line in lines:\n",
    "    id, text, phoneme, note, note_duration, phoneme_duration, slur_note = parser_line(line)\n",
    "    phoneme_set.update(set(phoneme))\n",
    "    note_set.update(set(note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 {'p', 'h', 'ie', 'e', 'SP', 'un', 'c', 'o', 'g', 'u', 'van', 'd', 'i', 'y', 'en', 'ing', 'ai', 'ei', 's', 'ch', 'ia', 'iong', 'uang', 'f', 'r', 'm', 'q', 'iu', 'ong', 'b', 'an', 'eng', 'k', 't', 'zh', 'v', 'a', 'vn', 'uai', 'ou', 'ian', 'ao', 'x', 'sh', 'uo', 'j', 'AP', 'ui', 'iang', 'er', 'iao', 'ang', 'l', 'n', 'in', 'ua', 'z', 'w', 'uan', 've'}\n",
      "35 {'F#4/Gb4', 'G4', 'D2', 'F4', 'A5', 'C3', 'C4', 'F#3/Gb3', 'D3', 'rest', 'G#4/Ab4', 'D5', 'C#3/Db3', 'B4', 'C#2/Db2', 'D#5/Eb5', 'C5', 'G#3/Ab3', 'D#4/Eb4', 'A4', 'E3', 'F3', 'C#4/Db4', 'F#5/Gb5', 'A#3/Bb3', 'E4', 'A#4/Bb4', 'C#5/Db5', 'E5', 'F5', 'A3', 'G3', 'B3', 'D4', 'D#3/Eb3'}\n"
     ]
    }
   ],
   "source": [
    "print_all([phoneme_set,note_set])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打一个示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2001000006|漂浮在一片无奈|p iao f u z ai ai ai AP SP y i i p ian ian ian w u n ai SP AP|E4 E4 F#4/Gb4 F#4/Gb4 G#4/Ab4 G#4/Ab4 A4 G#4/Ab4 rest rest E4 E4 F#4/Gb4 G#4/Ab4 G#4/Ab4 A4 G#4/Ab4 E4 E4 F#4/Gb4 F#4/Gb4 rest rest|0.185230 0.185230 0.177410 0.177410 0.193930 0.193930 0.259670 0.299340 0.215550 0.031770 0.197520 0.197520 0.165450 0.184760 0.184760 0.212290 0.246960 0.440370 0.440370 1.524950 1.524950 0.855830 0.559100|0.06011 0.12512 0.07517 0.10224 0.08603 0.1079 0.25967 0.29934 0.21555 0.03177 0.05175 0.14577 0.16545 0.0748 0.10996 0.21229 0.24696 0.09617 0.3442 0.1437 1.38125 0.85583 0.5591|0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = lines[5]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 2001000006\n",
      "7 漂浮在一片无奈\n",
      "23 ['p', 'iao', 'f', 'u', 'z', 'ai', 'ai', 'ai', 'AP', 'SP', 'y', 'i', 'i', 'p', 'ian', 'ian', 'ian', 'w', 'u', 'n', 'ai', 'SP', 'AP']\n",
      "23 ['E4', 'E4', 'F#4/Gb4', 'F#4/Gb4', 'G#4/Ab4', 'G#4/Ab4', 'A4', 'G#4/Ab4', 'rest', 'rest', 'E4', 'E4', 'F#4/Gb4', 'G#4/Ab4', 'G#4/Ab4', 'A4', 'G#4/Ab4', 'E4', 'E4', 'F#4/Gb4', 'F#4/Gb4', 'rest', 'rest']\n",
      "23 [0.18523, 0.18523, 0.17741, 0.17741, 0.19393, 0.19393, 0.25967, 0.29934, 0.21555, 0.03177, 0.19752, 0.19752, 0.16545, 0.18476, 0.18476, 0.21229, 0.24696, 0.44037, 0.44037, 1.52495, 1.52495, 0.85583, 0.5591]\n",
      "23 [0.06011, 0.12512, 0.07517, 0.10224, 0.08603, 0.1079, 0.25967, 0.29934, 0.21555, 0.03177, 0.05175, 0.14577, 0.16545, 0.0748, 0.10996, 0.21229, 0.24696, 0.09617, 0.3442, 0.1437, 1.38125, 0.85583, 0.5591]\n",
      "23 [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "id, text, phoneme, note, note_duration, phoneme_duration, slur_note = parser_line(line)\n",
    "print_all([id, text, phoneme, note, note_duration, phoneme_duration, slur_note])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "音频文件读取测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 92003])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform = get_audio(id, path)\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "汉字元音辅音组合为单个汉字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'u', 'v', 'a', 'ia', 'ua', 'o', 'uo', 'e', 'ie', 've', 'ai', 'uai', 'ei', 'uei', 'ao', 'iao', 'ou', 'iou', 'an', 'ian', 'uan', 'van', 'en', 'in', 'uen', 'vn', 'ang', 'iang', 'uang', 'eng', 'ing', 'ueng', 'ong', 'iong', 'er', 'ê']\n"
     ]
    }
   ],
   "source": [
    "print(get_final_table())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_note(text, phoneme, note, note_duration, slur_note):\n",
    "    # 1. check whether the phoneme is in finals\n",
    "    INITIALS = get_initial_table()\n",
    "    FINALS = get_final_table()\n",
    "    # is_final = [1 if p in FINALS else 0 for p in phoneme]\n",
    "    phoneme = phoneme.copy()\n",
    "    note = note.copy()\n",
    "    note_duration = note_duration.copy()\n",
    "    slur_note = slur_note.copy()\n",
    "    j = -1\n",
    "    text+='////////////////////'\n",
    "    text_with_p = phoneme.copy()\n",
    "    used_flag = False\n",
    "    for i in range(len(text_with_p)):\n",
    "        if text_with_p[i] in ['AP', 'SP']:\n",
    "            continue\n",
    "        if j==-1 or phoneme[i] in INITIALS or (phoneme[i-1] not in INITIALS and phoneme[i] != phoneme[i-1]):\n",
    "            j+=1\n",
    "            used_flag = False\n",
    "        text_with_p[i] = text[j] if used_flag == False else '~'\n",
    "        used_flag = True\n",
    "    for i in range(len(phoneme)-1, 0, -1):\n",
    "        if (note_duration[i] == note_duration[i-1] and phoneme[i-1] in INITIALS):\n",
    "            del note_duration[i]\n",
    "            del note[i]\n",
    "            phoneme[i-1]=[phoneme[i-1],phoneme[i]]\n",
    "            del phoneme[i]\n",
    "            del text_with_p[i]\n",
    "            del slur_note[i]\n",
    "        elif phoneme[i] in FINALS or phoneme[i] in ['AP', 'SP']:\n",
    "            phoneme[i] = [phoneme[i]]\n",
    "    return text_with_p, phoneme, note, note_duration, slur_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_note_old(text, phoneme, note, note_duration):\n",
    "    # remove the duplicate items in phoneme, note, and note_duration\n",
    "    # use text to verify the length\n",
    "    phoneme = phoneme.copy()\n",
    "    note = note.copy()\n",
    "    note_duration = note_duration.copy()\n",
    "    j = -1\n",
    "    text+='////////////////////'\n",
    "    text_with_p = phoneme.copy()\n",
    "    used_flag = False\n",
    "    for i in range(len(text_with_p)):\n",
    "        if text_with_p[i] in ['AP', 'SP']:\n",
    "            continue\n",
    "        if j==-1 or phoneme[i] in initial_table or (phoneme[i-1] not in initial_table and phoneme[i] != phoneme[i-1]):\n",
    "            j+=1\n",
    "            used_flag = False\n",
    "        text_with_p[i] = text[j] if used_flag == False else '~'\n",
    "        used_flag = True\n",
    "    for i in range(len(phoneme)-1, 0, -1):\n",
    "        if (note_duration[i] == note_duration[i-1] and phoneme[i-1] in initial_table):\n",
    "            del note_duration[i]\n",
    "            del note[i]\n",
    "            phoneme[i-1]=phoneme[i-1]+phoneme[i]\n",
    "            del phoneme[i]\n",
    "            del text_with_p[i]\n",
    "    return text_with_p, phoneme, note, note_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 ['漂', '浮', '在', '~', '~', 'AP', 'SP', '一', '~', '片', '~', '~', '无', '奈', 'SP', 'AP']\n",
      "16 [['p', 'iao'], ['f', 'u'], ['z', 'ai'], ['ai'], ['ai'], ['AP'], ['SP'], ['y', 'i'], ['i'], ['p', 'ian'], ['ian'], ['ian'], ['w', 'u'], ['n', 'ai'], ['SP'], ['AP']]\n",
      "16 ['E4', 'F#4/Gb4', 'G#4/Ab4', 'A4', 'G#4/Ab4', 'rest', 'rest', 'E4', 'F#4/Gb4', 'G#4/Ab4', 'A4', 'G#4/Ab4', 'E4', 'F#4/Gb4', 'rest', 'rest']\n",
      "16 [0.18523, 0.17741, 0.19393, 0.25967, 0.29934, 0.21555, 0.03177, 0.19752, 0.16545, 0.18476, 0.21229, 0.24696, 0.44037, 1.52495, 0.85583, 0.5591]\n",
      "16 [0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print_all(merge_note(text, phoneme, note, note_duration, slur_note))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import SpeechDataset\n",
    "from torch.utils.data import DataLoader, Dataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpencpopDataset(SpeechDataset):\n",
    "\n",
    "    def __init__(self, data_path, sample_rate=16000, transform=None):\n",
    "        super().__init__(data_path, sample_rate, transform)\n",
    "        transcript_file = data_path+'transcriptions.txt'\n",
    "        self.transcript = self.gen_transcript(transcript_file)\n",
    "        self.dataset_file_num = len(self.transcript)\n",
    "        self.threshold = 120000 # to avoid GPU memory used out\n",
    "        self.batch_size = 80 # to avoid GPU memory used out\n",
    "        self.split_ratio = [1000, 3]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_file_num\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        if idx >= self.dataset_file_num:\n",
    "            return {'audio': None, 'text': None}\n",
    "        line = self.transcript[idx]\n",
    "        id, text, phoneme, note, note_duration, phoneme_duration, slur_note = self.parser_line(line)\n",
    "        waveform = self.get_audio(id)\n",
    "        # text_with_p, phoneme, note, note_duration = merge_note(text, phoneme, note, note_duration)\n",
    "        sample = {'audio': waveform, 'text': line}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample, self.sample_rate)\n",
    "        return sample\n",
    "\n",
    "    def get_audio(self, id):\n",
    "        wav_path = self.data_path+'wavs/'+str(id)+'.wav'\n",
    "        waveform, sample_rate = torchaudio.load(wav_path)\n",
    "        if sample_rate != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform[0].unsqueeze(0), sample_rate, self.sample_rate)\n",
    "        return waveform\n",
    "\n",
    "    def parser_line(self, line):\n",
    "        id, text, phoneme, note, note_duration, phoneme_duration, slur_note = line.split('|')\n",
    "        phoneme = phoneme.split(' ')\n",
    "        note = note.split(' ')\n",
    "        note_duration = [float(i) for i in note_duration.split(' ')]\n",
    "        phoneme_duration = [float(i) for i in phoneme_duration.split(' ')]\n",
    "        slur_note = [int(i) for i in slur_note.split(' ')]\n",
    "        assert len(phoneme) == len(note_duration) and len(phoneme_duration) == len(slur_note) and len(slur_note) == len(phoneme)\n",
    "        return id, text, phoneme, note, note_duration, phoneme_duration, slur_note\n",
    "\n",
    "    def gen_transcript(self, transcript_file):\n",
    "        with open(transcript_file) as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            if (lines[-1]==''):\n",
    "                lines = lines[:-1]\n",
    "            return lines\n",
    "\n",
    "    def split(self, split_ratio=None, seed=42):\n",
    "        audio_dataset = self\n",
    "        size = len(audio_dataset)\n",
    "        my_split_ratio = self.split_ratio if split_ratio is None else split_ratio\n",
    "        lengths = [(i*size)//sum(my_split_ratio) for i in my_split_ratio]\n",
    "        lengths[-1] = size - sum(lengths[:-1])\n",
    "        split_dataset = random_split(audio_dataset, lengths, generator=torch.Generator().manual_seed(seed))\n",
    "        return split_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dataset_transform(sample, sample_rate=None):\n",
    "    id, text, phoneme, note, note_duration, phoneme_duration, slur_note = parser_line(sample['text'])\n",
    "    text_with_p, phoneme, note, note_duration, slur_note = merge_note(text, phoneme, note, note_duration, slur_note)\n",
    "    sample['chinese'] = text_with_p\n",
    "    sample['phoneme'] = phoneme\n",
    "    sample['note'] = note\n",
    "    sample['duration'] = note_duration\n",
    "    sample['slur'] = slur_note\n",
    "    return sample\n",
    "\n",
    "dataset = OpencpopDataset('/scratch/bh2283/data/opencpop/segments/', transform=dataset_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 tensor([[-1.8905e-03, -2.8823e-03, -2.3118e-03,  ...,  5.2431e-05,\n",
      "          2.1897e-04,  1.8222e-05]])\n",
      "1119 2001000019|宇宙磅礡而冷漠我们的爱微小却闪烁|y v zh ou b ang ang b o er l eng eng m o o o AP w o m en d e ai w ei x iao q ve sh an sh ou uo uo AP SP|C#4/Db4 C#4/Db4 D#4/Eb4 D#4/Eb4 C#4/Db4 C#4/Db4 D#4/Eb4 E4 E4 D#4/Eb4 E4 E4 F#4/Gb4 G#4/Ab4 G#4/Ab4 A4 G#4/Ab4 rest C#4/Db4 C#4/Db4 C#4/Db4 C#4/Db4 D#4/Eb4 D#4/Eb4 C#4/Db4 D#4/Eb4 D#4/Eb4 E4 E4 D#4/Eb4 D#4/Eb4 E4 E4 G#4/Ab4 G#4/Ab4 A4 G#4/Ab4 rest rest|0.194490 0.194490 0.191880 0.191880 0.219800 0.219800 0.138290 0.170840 0.170840 0.204960 0.131260 0.131260 0.219430 0.183230 0.183230 0.197770 0.379730 0.380810 0.203550 0.203550 0.165270 0.165270 0.141470 0.141470 0.159550 0.111580 0.111580 0.246980 0.246980 0.126240 0.126240 0.329620 0.329620 0.306020 0.306020 0.171160 0.302490 0.236470 0.095710|0.05355 0.14094 0.06248 0.1294 0.06762 0.15218 0.13829 0.06402 0.10682 0.20496 0.0324 0.09886 0.21943 0.07985 0.10338 0.19777 0.37973 0.38081 0.07355 0.13 0.0711 0.09417 0.05535 0.08612 0.15955 0.03354 0.07804 0.09701 0.14997 0.05886 0.06738 0.11824 0.21138 0.15725 0.14877 0.17116 0.30249 0.23647 0.09571|0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0\n",
      "25 ['宇', '宙', '磅', '~', '礡', '而', '冷', '~', '漠', '~', '~', 'AP', '我', '们', '的', '爱', '微', '小', '却', '闪', '烁', '/', '~', 'AP', 'SP']\n",
      "25 [['y', 'v'], ['zh', 'ou'], ['b', 'ang'], ['ang'], ['b', 'o'], ['er'], ['l', 'eng'], ['eng'], ['m', 'o'], ['o'], ['o'], ['AP'], ['w', 'o'], ['m', 'en'], ['d', 'e'], ['ai'], ['w', 'ei'], ['x', 'iao'], ['q', 've'], ['sh', 'an'], ['sh', 'ou'], ['uo'], ['uo'], ['AP'], ['SP']]\n",
      "25 ['C#4/Db4', 'D#4/Eb4', 'C#4/Db4', 'D#4/Eb4', 'E4', 'D#4/Eb4', 'E4', 'F#4/Gb4', 'G#4/Ab4', 'A4', 'G#4/Ab4', 'rest', 'C#4/Db4', 'C#4/Db4', 'D#4/Eb4', 'C#4/Db4', 'D#4/Eb4', 'E4', 'D#4/Eb4', 'E4', 'G#4/Ab4', 'A4', 'G#4/Ab4', 'rest', 'rest']\n",
      "25 [0.19449, 0.19188, 0.2198, 0.13829, 0.17084, 0.20496, 0.13126, 0.21943, 0.18323, 0.19777, 0.37973, 0.38081, 0.20355, 0.16527, 0.14147, 0.15955, 0.11158, 0.24698, 0.12624, 0.32962, 0.30602, 0.17116, 0.30249, 0.23647, 0.09571]\n",
      "25 [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print_all(dataset[18].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3744, 12)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set, test_set = dataset.split()\n",
    "len(train_set), len(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单个字测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(waveform):\n",
    "    torchaudio.save('./audio-temp.wav', waveform.unsqueeze(0), 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 107614])\n",
      "也 0.42954 ye\n"
     ]
    }
   ],
   "source": [
    "def test_one_char(data, idx, sample_rate = 16000):\n",
    "    assert idx < len(data['chinese'])\n",
    "    chinese = data['chinese'][idx]\n",
    "    time = data['duration'][idx]\n",
    "    duration = data['duration']\n",
    "    for i in range(1, len(duration)):\n",
    "        duration[i] += duration[i-1]\n",
    "    start = 0 if idx == 0 else int(duration[idx-1]*sample_rate)\n",
    "    end = int(duration[idx]*sample_rate)\n",
    "    print(data['audio'].shape)\n",
    "    waveform = data['audio'][0, start: end]\n",
    "    print(chinese, time, ''.join(data['phoneme'][idx]))\n",
    "    play(waveform)\n",
    "\n",
    "test_one_char(train_set[2], 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "靠耳朵听，发现大部分的标注是准确的，也有小部分划分有点出入。\n",
    "文本注音也有些问题，但是问题不大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader\n",
    "\n",
    "这儿我们制作一个单个字的dataloader，包含拼音、音调、时常（量化后），延音。\n",
    "\n",
    "之后使用lookup emb制作decoder的输入emb。这儿注意可以使用前一个字后一个字来辅助生成更好的声音。如果这样做，注意前一个字，本字，后一个字都需要使用不同的lookup table。\n",
    "\n",
    "先做一个naive版本，前后一个字不考虑。即使如此，我们的dataloader任然需要提供所有信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicLoaderGenerator:\n",
    "    def __init__(self, \n",
    "        labels, k_size=0, \n",
    "        num_workers=0,\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        ) -> None:\n",
    "        self.k_size = k_size\n",
    "        self.labels = labels\n",
    "        self.look_up = {s: i for i, s in enumerate(labels)}\n",
    "        self.device = device\n",
    "        self.num_workers = num_workers\n",
    "        self.version = '0.02'\n",
    "\n",
    "    def label2id(self, str):\n",
    "        return [self.look_up[i] for i in str]\n",
    "\n",
    "    def id2label(self, idcs):\n",
    "        return ''.join([self.labels[i] for i in idcs])\n",
    "\n",
    "    def batch_filter(self, batch:list):\n",
    "        # remove all audio with tag if audio length > threshold\n",
    "        for i in range(len(batch)-1, -1, -1):\n",
    "            if batch[i]['audio'].shape[-1] > self.threshold:\n",
    "                del batch[i]\n",
    "        return batch\n",
    "\n",
    "    def collate_wrapper(self, batch:list): # RAW\n",
    "        batch = self.batch_filter(batch)\n",
    "        bs = len(batch)\n",
    "        rand_shift = torch.randint(self.k_size, (bs,))\n",
    "        audio_list = [batch[i]['audio'][:,rand_shift[i]:] for i in range(bs)]\n",
    "        audio_length = [audio.shape[-1] for audio in audio_list]\n",
    "        target_list = [self.label2id(item['text']) for item in batch]\n",
    "        target_length = [len(l) for l in target_list]\n",
    "        chinese_list = [batch[i]['chinese'] for i in range(bs)]\n",
    "\n",
    "        target_length, target_list, audio_length, audio_list, chinese_list = zip(*sorted(zip(target_length, target_list, audio_length, audio_list, chinese_list), reverse=True))\n",
    "        target_length = torch.tensor(target_length)\n",
    "        audio_length = torch.tensor(audio_length)\n",
    "\n",
    "        max_audio_length = torch.max(audio_length)\n",
    "        audio_list = torch.cat([\n",
    "            torch.cat(\n",
    "            (audio, torch.zeros(max_audio_length-audio.shape[-1]).unsqueeze(0)), -1)\n",
    "            for audio in audio_list], 0)\n",
    "        \n",
    "        max_target_length = torch.max(target_length)\n",
    "        target_list = torch.cat([\n",
    "            torch.cat(\n",
    "            (torch.tensor(l), torch.zeros([max_target_length-len(l)], dtype=torch.int)), -1).unsqueeze(0) \n",
    "            for l in target_list], 0)\n",
    "        return {'audio': audio_list, 'audio_len': audio_length, \n",
    "                'target': target_list, 'target_len': target_length,\n",
    "                'chinese': chinese_list}\n",
    "\n",
    "    def dataloader(self, audioDataset, batch_size, shuffle=True):\n",
    "        # k_size is the kernel size for the encoder, for data augmentation\n",
    "        self.threshold = audioDataset.dataset.threshold\n",
    "        return DataLoader(audioDataset, batch_size,\n",
    "                            shuffle, num_workers=self.num_workers, collate_fn=self.collate_wrapper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaderGenerator = MusicLoaderGenerator(labels, k_size=5)\n",
    "train_loader = loaderGenerator.dataloader(train_set, batch_size=8)\n",
    "print('train_set:', len(train_set), 'test_set:',len(test_set))\n",
    "    steps = 10\n",
    "    for i_batch, sample_batched in enumerate(train_loader):\n",
    "        if steps <= 0:\n",
    "            break\n",
    "        print(sample_batched['audio'].shape, sample_batched['target'].shape)\n",
    "        print(sample_batched['audio_len'], sample_batched['target_len'])\n",
    "        steps -= 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3322949f56cb4db99427e05ed2d4a87f0497ffa3e41dd81b99d577253bd3be5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
